{
  "examples": [
    {
      "id": "001-basic-generation",
      "title": "Simple text generation",
      "description": "",
      "order": 1,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 2,
          "line_range": [
            2,
            2
          ]
        },
        {
          "code": "# Our first example demonstrates how to use the Gemini API\n# to generate content with a simple prompt.\n",
          "display_code": "",
          "annotation": "Our first example demonstrates how to use the Gemini API\nto generate content with a simple prompt.",
          "is_comment": true,
          "start_line": 3,
          "line_range": [
            3,
            4
          ],
          "target_line_range": [
            5,
            6
          ]
        },
        {
          "code": "from google import genai\n\n",
          "display_code": "from google import genai\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            6
          ]
        },
        {
          "code": "# Best practice: store your API key in an environment variable\n# and load it from there.\n",
          "display_code": "",
          "annotation": "Best practice: store your API key in an environment variable\nand load it from there.",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            8
          ],
          "target_line_range": [
            9,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n)\nprint(response.text)\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n)\nprint(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 9,
          "line_range": [
            9,
            14
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python basic-generation.py",
          "output": "AI works by learning patterns from data, then using those patterns to make predictions or generate new content. It processes information through neural networks that mimic human brain connections, identifying features and relationships to perform tasks like recognition, prediction, and generation."
        }
      ],
      "image_data": [
        {
          "path": "examples/001-basic-generation/basic-generation.png",
          "filename": "basic-generation.png",
          "caption": "Generation"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/text-generation"
      ],
      "section_id": "001-basic-text",
      "section_title": "Text"
    },
    {
      "id": "002-streaming-text",
      "title": "Streaming text",
      "description": "This example demonstrates how to use the Gemini API to generate text content and stream the output.",
      "order": 2,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            6
          ]
        },
        {
          "code": "from google import genai\n\n",
          "display_code": "from google import genai\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            6
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            7
          ],
          "target_line_range": [
            8,
            9
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 8,
          "line_range": [
            8,
            9
          ]
        },
        {
          "code": "# Call the API to generate content in streaming mode\n",
          "display_code": "",
          "annotation": "Call the API to generate content in streaming mode",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            15
          ]
        },
        {
          "code": "response = client.models.generate_content_stream(\n    model=\"gemini-2.0-flash\",\n    contents=[\"Explain how AI works\"]\n)\n\n",
          "display_code": "response = client.models.generate_content_stream(\n    model=\"gemini-2.0-flash\",\n    contents=[\"Explain how AI works\"]\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            15
          ]
        },
        {
          "code": "# Iterate over the stream of responses and print each chunk of text\n",
          "display_code": "",
          "annotation": "Iterate over the stream of responses and print each chunk of text",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            18
          ]
        },
        {
          "code": "for chunk in response:\n    print(chunk.text, end=\"\")\n",
          "display_code": "for chunk in response:\n    print(chunk.text, end=\"\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            18
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python streaming-generation.py",
          "output": "AI, or Artificial Intelligence, is a broad field of computer science focused on creating machines capable of performing tasks that typically require human intelligence. It involves developing algorithms and models that enable computers to learn from data, reason, solve problems, understand natural language, perceive their environment, and make decisions.\nAI can be achieved through various techniques, including:\n*   **Machine Learning (ML):** This is a core subfield of AI where machines learn from data without being explicitly programmed. ML algorithms can identify patterns, make predictions, and improve their performance over time with more data.\n*   **Deep Learning (DL):** A subfield of ML that uses artificial neural networks with multiple layers (deep neural networks) to analyze data and extract complex features. DL has been highly successful in areas like image recognition, natural language processing, and speech recognition.\n*   **Natural Language Processing (NLP):** Focuses on enabling computers to understand, interpret, and generate human language. NLP techniques are used in applications like chatbots, machine translation, and sentiment analysis.\n*   **Computer Vision:** Enables computers to \"see\" and interpret images and videos. Computer vision algorithms can identify objects, recognize faces, and analyze scenes.\n*   **Robotics:** Involves designing, constructing, operating, and applying robots. AI is often used in robotics to enable robots to perform tasks autonomously.\nAI is transforming various industries, including healthcare, finance, transportation, and manufacturing. It has the potential to solve complex problems and improve people's lives, but it also raises ethical and societal concerns that need to be addressed."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/text-generation"
      ],
      "section_id": "001-basic-text",
      "section_title": "Text"
    },
    {
      "id": "003-system-prompt",
      "title": "System prompt",
      "description": "This example demonstrates how to use system instructions to guide the model's behavior.",
      "order": 3,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            7
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            7
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 8,
          "line_range": [
            8,
            8
          ],
          "target_line_range": [
            9,
            10
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 9,
          "line_range": [
            9,
            10
          ]
        },
        {
          "code": "# Configure the model with system instructions\n# These instructions tell the model to act as a pirate\n",
          "display_code": "",
          "annotation": "Configure the model with system instructions\nThese instructions tell the model to act as a pirate",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            12
          ],
          "target_line_range": [
            13,
            19
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    config=types.GenerateContentConfig(\n        system_instruction=\"You are a pirate.  Answer all questions like a pirate.\"),\n    contents=\"Hello there\"\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    config=types.GenerateContentConfig(\n        system_instruction=\"You are a pirate.  Answer all questions like a pirate.\"),\n    contents=\"Hello there\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            19
          ]
        },
        {
          "code": "# Print the model's response\n",
          "display_code": "",
          "annotation": "Print the model's response",
          "is_comment": true,
          "start_line": 20,
          "line_range": [
            20,
            20
          ],
          "target_line_range": [
            21,
            21
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 21,
          "line_range": [
            21,
            21
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python system_instructions.py",
          "output": "Ahoy there, matey! What be on yer mind?"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/text-generation#system-instructions"
      ],
      "section_id": "001-basic-text",
      "section_title": "Text"
    },
    {
      "id": "019-reasoning-models",
      "title": "Reasoning models",
      "description": "This example demonstrates how to access the reasoning trace of a Gemini model\nand then the final text output.\nReasoning models are a new type of model that 'think' a little bit before\ngiving a final answer. The 'thinking' response is visible in Google AI Studio\nbut not as part of the response to an API call.",
      "order": 19,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 7,
          "line_range": [
            7,
            7
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 8,
          "line_range": [
            8,
            8
          ],
          "target_line_range": [
            9,
            11
          ]
        },
        {
          "code": "from google import genai\nimport os\n\n",
          "display_code": "from google import genai\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 9,
          "line_range": [
            9,
            11
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Define a profound question about the universe\n",
          "display_code": "",
          "annotation": "Define a profound question about the universe",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            17
          ]
        },
        {
          "code": "prompt = \"If the universe is expanding, what is it expanding into? Show your reasoning.\"\n\n",
          "display_code": "prompt = \"If the universe is expanding, what is it expanding into? Show your reasoning.\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            17
          ]
        },
        {
          "code": "# Generate content with the Gemini model\n",
          "display_code": "",
          "annotation": "Generate content with the Gemini model",
          "is_comment": true,
          "start_line": 18,
          "line_range": [
            18,
            18
          ],
          "target_line_range": [
            19,
            24
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=prompt,\n)\n\nprint(response.text)\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\",\n    contents=prompt,\n)\n\nprint(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 19,
          "line_range": [
            19,
            24
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python universe_reasoning.py",
          "output": "This is a fantastic and very common question that gets to the heart of how we understand the universe based on Einstein's theory of General Relativity. The most accurate answer, according to our current understanding, is:\n**The universe isn't expanding *into* anything. Space itself is expanding.**\nHere's the reasoning:\n1.  **General Relativity's View of Spacetime:** Our everyday intuition thinks of space as a pre-existing, static container \u2013 like an empty room that things can move around *in*. General Relativity, however, describes spacetime not as a fixed background but as a dynamic entity. It can warp, bend (which we experience as gravity), and, crucially, *expand or contract*.\n2.  **Expansion is Intrinsic:** The expansion of the universe isn't like an explosion *within* a pre-existing void, where debris flies outwards *into* empty space. Instead, it's the very fabric of spacetime *itself* that is stretching. Imagine the space *between* galaxies is growing.\n3.  **Analogies (and their limitations):**\n    *   **The Rising Raisin Bread:** Imagine raisins (representing galaxies) embedded in dough (representing space). As the dough bakes and expands, all the raisins move further apart from each other. A raisin doesn't see itself as being at the center; it sees all other raisins moving away from it. Importantly, the *dough itself* is expanding.\n        *   *Limitation:* This analogy breaks down because the dough has edges and is expanding *into* the oven (an external space). Our universe, as far as we know, doesn't have an edge or an \"outside.\"\n    *   **The Expanding Balloon Surface:** Imagine drawing dots (galaxies) on the surface of a balloon. As you inflate the balloon, the rubber (space) stretches, and the distance between any two dots on the surface increases. From the perspective of any dot, all other dots are moving away. There is no \"center\" of expansion *on the surface* itself.\n        *   *Limitation:* The 2D surface of the balloon is expanding *into* the 3D space around it. General Relativity doesn't require our 3D space to be expanding into a higher-dimensional \"hyperspace.\" The expansion is an intrinsic property of our spacetime dimensions.\n4.  **No Center, No Edge:** Because space *itself* is expanding everywhere, there isn't a central point *from which* the universe is expanding. The Big Bang wasn't an explosion *at* a point *in* space; it was the beginning of the expansion *of* space, happening everywhere simultaneously. Consequently, there's no \"edge\" of the universe expanding outwards into a void.\n5.  **Infinite or Finite?**\n    *   If the universe is *infinite*, then it was always infinite. An expanding infinite universe simply becomes \"more infinite\" \u2013 the distances between objects grow, but it isn't expanding *into* anything because there's no outside to an infinite space.\n    *   If the universe is *finite* but unbounded (like the surface of the balloon, but in 3D), its total volume increases, but it still doesn't require an external space to expand into. It's self-contained.\n**In summary:** The concept of \"expanding into\" relies on the idea of an external space or container. According to General Relativity, the universe *is* the container (spacetime), and it's this container itself that is growing. There is no need for an \"outside\" for this expansion to occur."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/thinking"
      ],
      "section_id": "001-basic-text",
      "section_title": "Text"
    },
    {
      "id": "020-structured-output",
      "title": "Structured output",
      "description": "This example shows how to generate structured data using a pydantic model to represent Cats with name, colour, and special ability.",
      "order": 20,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API and pydantic\n",
          "display_code": "",
          "annotation": "Import the Gemini API and pydantic",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom pydantic import BaseModel\nimport os\n\n\n",
          "display_code": "from google import genai\nfrom pydantic import BaseModel\nimport os\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Define a Pydantic model for a Cat\n",
          "display_code": "",
          "annotation": "Define a Pydantic model for a Cat",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            16
          ]
        },
        {
          "code": "class Cat(BaseModel):\n    name: str\n    colour: str\n    special_ability: str\n\n\n",
          "display_code": "class Cat(BaseModel):\n    name: str\n    colour: str\n    special_ability: str\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            16
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 17,
          "line_range": [
            17,
            17
          ],
          "target_line_range": [
            18,
            19
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            19
          ]
        },
        {
          "code": "# Define the prompt. Note: It asks for 3 cats\n",
          "display_code": "",
          "annotation": "Define the prompt. Note: It asks for 3 cats",
          "is_comment": true,
          "start_line": 20,
          "line_range": [
            20,
            20
          ],
          "target_line_range": [
            21,
            22
          ]
        },
        {
          "code": "prompt = \"Generate data for 3 cats, including their name, colour and special ability.\"\n\n",
          "display_code": "prompt = \"Generate data for 3 cats, including their name, colour and special ability.\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 21,
          "line_range": [
            21,
            22
          ]
        },
        {
          "code": "# Call the API to generate content, specifying the response schema.\n# Note that it expects a `list` and not a `typing.List` object.\n# For some reason Gemini models are finicky about that.\n",
          "display_code": "",
          "annotation": "Call the API to generate content, specifying the response schema.\nNote that it expects a `list` and not a `typing.List` object.\nFor some reason Gemini models are finicky about that.",
          "is_comment": true,
          "start_line": 23,
          "line_range": [
            23,
            25
          ],
          "target_line_range": [
            26,
            34
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=prompt,\n    config={\n        \"response_mime_type\": \"application/json\",\n        \"response_schema\": list[Cat],\n    },\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=prompt,\n    config={\n        \"response_mime_type\": \"application/json\",\n        \"response_schema\": list[Cat],\n    },\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 26,
          "line_range": [
            26,
            34
          ]
        },
        {
          "code": "# Parse the json response to a list of Cat objects\n",
          "display_code": "",
          "annotation": "Parse the json response to a list of Cat objects",
          "is_comment": true,
          "start_line": 35,
          "line_range": [
            35,
            35
          ],
          "target_line_range": [
            36,
            37
          ]
        },
        {
          "code": "my_cats: list[Cat] = response.parsed\n\n",
          "display_code": "my_cats: list[Cat] = response.parsed\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 36,
          "line_range": [
            36,
            37
          ]
        },
        {
          "code": "# Print the generated cat data\n",
          "display_code": "",
          "annotation": "Print the generated cat data",
          "is_comment": true,
          "start_line": 38,
          "line_range": [
            38,
            38
          ],
          "target_line_range": [
            39,
            42
          ]
        },
        {
          "code": "for cat in my_cats:\n    print(\n        f\"Name: {cat.name}, Colour: {cat.colour}, Special Ability: {cat.special_ability}\"\n    )\n",
          "display_code": "for cat in my_cats:\n    print(\n        f\"Name: {cat.name}, Colour: {cat.colour}, Special Ability: {cat.special_ability}\"\n    )\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 39,
          "line_range": [
            39,
            42
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and pydantic",
          "command": "pip install google-genai pydantic",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python structured_cats.py",
          "output": "Name: Aria, Colour: tortoiseshell, Special Ability: Can teleport short distances\nName: Blupus, Colour: ginger, Special Ability: Understands human speech\nName: Moonshine, Colour: black and white, Special Ability: Invisible at night"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/structured-output?lang=python",
        "https://ai.google.dev/gemini-api/docs/structured-output?lang=rest"
      ],
      "section_id": "001-basic-text",
      "section_title": "Text"
    },
    {
      "id": "004-image-q-a",
      "title": "Image question answering",
      "description": "This example demonstrates how to use the Gemini API to analyze or understand images of cats, including using image URLs and base64 encoding.",
      "order": 4,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries\n",
          "display_code": "",
          "annotation": "Import necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport requests\nimport base64\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport requests\nimport base64\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Replace with your Gemini API key\n",
          "display_code": "",
          "annotation": "Replace with your Gemini API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# We'll start by using an image URL.\n# Load an image of a cat from a URL\n",
          "display_code": "",
          "annotation": "We'll start by using an image URL.\nLoad an image of a cat from a URL",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            14
          ],
          "target_line_range": [
            15,
            18
          ]
        },
        {
          "code": "image_url = \"https://cataas.com/cat\"\nimage_response = requests.get(image_url)\nimage_content = image_response.content\n\n",
          "display_code": "image_url = \"https://cataas.com/cat\"\nimage_response = requests.get(image_url)\nimage_content = image_response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            18
          ]
        },
        {
          "code": "# Ask Gemini about the cat in the image\n",
          "display_code": "",
          "annotation": "Ask Gemini about the cat in the image",
          "is_comment": true,
          "start_line": 19,
          "line_range": [
            19,
            19
          ],
          "target_line_range": [
            20,
            26
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\"What breed of cat is this?\", types.Part.from_bytes(data=image_content, mime_type=\"image/jpeg\")]\n)\n\nprint(\"Response from URL Image:\\n\", response.text)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\"What breed of cat is this?\", types.Part.from_bytes(data=image_content, mime_type=\"image/jpeg\")]\n)\n\nprint(\"Response from URL Image:\\n\", response.text)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 20,
          "line_range": [
            20,
            26
          ]
        },
        {
          "code": "# Now we'll use a local image file.\n# Load a local image of a cat and encode it as Base64\n",
          "display_code": "",
          "annotation": "Now we'll use a local image file.\nLoad a local image of a cat and encode it as Base64",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            28
          ],
          "target_line_range": [
            29,
            31
          ]
        },
        {
          "code": "with open(\"cat.jpg\", \"rb\") as image_file:\n    encoded_string = base64.b64encode(image_file.read())\n\n",
          "display_code": "with open(\"cat.jpg\", \"rb\") as image_file:\n    encoded_string = base64.b64encode(image_file.read())\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 29,
          "line_range": [
            29,
            31
          ]
        },
        {
          "code": "# Ensure the encoded string is a string\n",
          "display_code": "",
          "annotation": "Ensure the encoded string is a string",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            32
          ],
          "target_line_range": [
            33,
            34
          ]
        },
        {
          "code": "encoded_string = encoded_string.decode('utf-8')\n\n",
          "display_code": "encoded_string = encoded_string.decode('utf-8')\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            34
          ]
        },
        {
          "code": "# Ask Gemini a question about the cat, providing the image as a Base64 string\n",
          "display_code": "",
          "annotation": "Ask Gemini a question about the cat, providing the image as a Base64 string",
          "is_comment": true,
          "start_line": 35,
          "line_range": [
            35,
            35
          ],
          "target_line_range": [
            36,
            41
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\"Is this cat fluffy?\", types.Part.from_bytes(data=base64.b64decode(encoded_string), mime_type=\"image/jpeg\")]\n)\n\nprint(\"\\nResponse from Base64 Image:\\n\", response.text)\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\"Is this cat fluffy?\", types.Part.from_bytes(data=base64.b64decode(encoded_string), mime_type=\"image/jpeg\")]\n)\n\nprint(\"\\nResponse from Base64 Image:\\n\", response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 36,
          "line_range": [
            36,
            41
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and requests",
          "command": "pip install google-genai requests",
          "output": ""
        },
        {
          "explanation": "Download an example cat image (replace with your own if needed)",
          "command": "wget https://cataas.com/cat -O cat.jpg",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python gemini-cat.py",
          "output": "Response from URL Image:\n This looks like a British Shorthair cat.\nResponse from Base64 Image:\n Yes, this cat appears to be fluffy."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python"
      ],
      "section_id": "002-basic-images",
      "section_title": "Images"
    },
    {
      "id": "005-image-generation",
      "title": "Image generation (Gemini and Imagen)",
      "description": "This example demonstrates generating images using both Gemini 2.0 Flash and Imagen 3 models, focusing on cat-related prompts.",
      "order": 5,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries. Make sure Pillow is installed!\n",
          "display_code": "",
          "annotation": "Import necessary libraries. Make sure Pillow is installed!",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nfrom PIL import Image\nfrom io import BytesIO\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nfrom PIL import Image\nfrom io import BytesIO\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# We start by using Gemini 2.0 Flash for image generation.\n# Demonstrates generating an image and associated text using Gemini 2.0 Flash.\n",
          "display_code": "",
          "annotation": "We start by using Gemini 2.0 Flash for image generation.\nDemonstrates generating an image and associated text using Gemini 2.0 Flash.",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            14
          ],
          "target_line_range": [
            15,
            25
          ]
        },
        {
          "code": "contents = (\n    \"Hi, can you create a 3D rendered image of a cat wearing a wizard hat, \"\n    \"casting a spell in a magical forest?\"\n)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-exp-image-generation\",\n    contents=contents,\n    config=types.GenerateContentConfig(response_modalities=[\"Text\", \"Image\"]),\n)\n\n",
          "display_code": "contents = (\n    \"Hi, can you create a 3D rendered image of a cat wearing a wizard hat, \"\n    \"casting a spell in a magical forest?\"\n)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-exp-image-generation\",\n    contents=contents,\n    config=types.GenerateContentConfig(response_modalities=[\"Text\", \"Image\"]),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            25
          ]
        },
        {
          "code": "# Save the image to a file and display it\n",
          "display_code": "",
          "annotation": "Save the image to a file and display it",
          "is_comment": true,
          "start_line": 26,
          "line_range": [
            26,
            26
          ],
          "target_line_range": [
            27,
            34
          ]
        },
        {
          "code": "for part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.save(\"gemini-cat-wizard.png\")\n        image.show()\n\n",
          "display_code": "for part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.save(\"gemini-cat-wizard.png\")\n        image.show()\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 27,
          "line_range": [
            27,
            34
          ]
        },
        {
          "code": "# Now we use Imagen 3 for image generation.\n# Demonstrates generating multiple high-quality images of cats using Imagen 3.\n# Imagen 3 is only available on the Paid Tier and always includes a SynthID watermark.\n",
          "display_code": "",
          "annotation": "Now we use Imagen 3 for image generation.\nDemonstrates generating multiple high-quality images of cats using Imagen 3.\nImagen 3 is only available on the Paid Tier and always includes a SynthID watermark.",
          "is_comment": true,
          "start_line": 35,
          "line_range": [
            35,
            37
          ],
          "target_line_range": [
            38,
            43
          ]
        },
        {
          "code": "response = client.models.generate_images(\n    model=\"imagen-3.0-generate-002\",\n    prompt=\"A photorealistic image of a cat astronaut floating in space\",\n    config=types.GenerateImagesConfig(number_of_images=2),\n)\n\n",
          "display_code": "response = client.models.generate_images(\n    model=\"imagen-3.0-generate-002\",\n    prompt=\"A photorealistic image of a cat astronaut floating in space\",\n    config=types.GenerateImagesConfig(number_of_images=2),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 38,
          "line_range": [
            38,
            43
          ]
        },
        {
          "code": "# Save the images to files and display them\n",
          "display_code": "",
          "annotation": "Save the images to files and display them",
          "is_comment": true,
          "start_line": 44,
          "line_range": [
            44,
            44
          ],
          "target_line_range": [
            45,
            48
          ]
        },
        {
          "code": "for i, generated_image in enumerate(response.generated_images):\n    image = Image.open(BytesIO(generated_image.image.image_bytes))\n    image.save(f\"imagen-cat-astronaut-{i+1}.png\")\n    image.show()\n",
          "display_code": "for i, generated_image in enumerate(response.generated_images):\n    image = Image.open(BytesIO(generated_image.image.image_bytes))\n    image.save(f\"imagen-cat-astronaut-{i+1}.png\")\n    image.show()\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 45,
          "line_range": [
            45,
            48
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai Pillow",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python image-generation.py",
          "output": "# Expected output (will vary based on the model):\n# (Text describing the cat wizard image from Gemini 2.0 Flash)\n# (Two image windows will open, displaying the generated cat astronaut images from Imagen 3)\n# Image saved as gemini-cat-wizard.png\n# (Two image windows will open, displaying the generated cat astronaut images from Imagen 3)"
        }
      ],
      "image_data": [
        {
          "path": "examples/005-image-generation/cat-wizard.png",
          "filename": "cat-wizard.png",
          "caption": "Wizard"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/image-generation"
      ],
      "section_id": "002-basic-images",
      "section_title": "Images"
    },
    {
      "id": "006-editing-images",
      "title": "Edit an image",
      "description": "This example demonstrates how to edit an existing image of a cat to add a hat using the Gemini API.",
      "order": 6,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries. Make sure Pillow is installed!\n",
          "display_code": "",
          "annotation": "Import necessary libraries. Make sure Pillow is installed!",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            11
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            11
          ]
        },
        {
          "code": "# Set your Gemini API key\n",
          "display_code": "",
          "annotation": "Set your Gemini API key",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Download the cat image from cataas.com\n",
          "display_code": "",
          "annotation": "Download the cat image from cataas.com",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            19
          ]
        },
        {
          "code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "display_code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            19
          ]
        },
        {
          "code": "# Prepare a prompt.\n",
          "display_code": "",
          "annotation": "Prepare a prompt.",
          "is_comment": true,
          "start_line": 20,
          "line_range": [
            20,
            20
          ],
          "target_line_range": [
            21,
            22
          ]
        },
        {
          "code": "text_prompt = \"Please add a stylish top hat to this cat.\"\n\n",
          "display_code": "text_prompt = \"Please add a stylish top hat to this cat.\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 21,
          "line_range": [
            21,
            22
          ]
        },
        {
          "code": "# Generate content with the Gemini API\n",
          "display_code": "",
          "annotation": "Generate content with the Gemini API",
          "is_comment": true,
          "start_line": 23,
          "line_range": [
            23,
            23
          ],
          "target_line_range": [
            24,
            30
          ]
        },
        {
          "code": "model = \"gemini-2.0-flash-exp-image-generation\"\nresponse = client.models.generate_content(\n    model=model,\n    contents=[text_prompt, cat_image],\n    config=types.GenerateContentConfig(response_modalities=[\"Text\", \"Image\"]),\n)\n\n",
          "display_code": "model = \"gemini-2.0-flash-exp-image-generation\"\nresponse = client.models.generate_content(\n    model=model,\n    contents=[text_prompt, cat_image],\n    config=types.GenerateContentConfig(response_modalities=[\"Text\", \"Image\"]),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 24,
          "line_range": [
            24,
            30
          ]
        },
        {
          "code": "# Process the response. Retry if you get a 500 error.\n",
          "display_code": "",
          "annotation": "Process the response. Retry if you get a 500 error.",
          "is_comment": true,
          "start_line": 31,
          "line_range": [
            31,
            31
          ],
          "target_line_range": [
            32,
            36
          ]
        },
        {
          "code": "for part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    elif part.inline_data is not None:\n        print(f\"Received {part.inline_data.mime_type} data\")\n",
          "display_code": "for part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    elif part.inline_data is not None:\n        print(f\"Received {part.inline_data.mime_type} data\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 32,
          "line_range": [
            32,
            36
          ]
        },
        {
          "code": "        # The data is provided as raw bytes, not base64-encoded\n",
          "display_code": "",
          "annotation": "The data is provided as raw bytes, not base64-encoded",
          "is_comment": true,
          "start_line": 37,
          "line_range": [
            37,
            37
          ],
          "target_line_range": [
            38,
            40
          ]
        },
        {
          "code": "        image = Image.open(BytesIO(part.inline_data.data))\n        image.save(\"cat_with_hat.png\")\n        print(\"\\nImage saved as cat_with_hat.png\")\n",
          "display_code": "        image = Image.open(BytesIO(part.inline_data.data))\n        image.save(\"cat_with_hat.png\")\n        print(\"\\nImage saved as cat_with_hat.png\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 38,
          "line_range": [
            38,
            40
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai Pillow requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python edit_cat.py",
          "output": "Image saved as cat_with_hat.png"
        }
      ],
      "image_data": [
        {
          "path": "examples/006-editing-images/cat_with_hat.png",
          "filename": "cat_with_hat.png",
          "caption": "cat_with_hat"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/image-generation#gemini-image-editing"
      ],
      "section_id": "002-basic-images",
      "section_title": "Images"
    },
    {
      "id": "007-bounding-boxes",
      "title": "Bounding boxes",
      "description": "This example demonstrates how to use the Gemini API to detect an object (a cat) in an image and retrieve its bounding box coordinates.",
      "order": 7,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries. Make sure Pillow is installed!\n",
          "display_code": "",
          "annotation": "Import necessary libraries. Make sure Pillow is installed!",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\n",
          "display_code": "from google import genai\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# Specify the prompt, asking for a bounding box around the cat\n",
          "display_code": "",
          "annotation": "Specify the prompt, asking for a bounding box around the cat",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            18
          ]
        },
        {
          "code": "prompt = (\n    \"Return a bounding box for the cat in this image \"\n    \"in [ymin, xmin, ymax, xmax] format.\"\n)\n\n",
          "display_code": "prompt = (\n    \"Return a bounding box for the cat in this image \"\n    \"in [ymin, xmin, ymax, xmax] format.\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            18
          ]
        },
        {
          "code": "# Download the cat image from cataas.com\n",
          "display_code": "",
          "annotation": "Download the cat image from cataas.com",
          "is_comment": true,
          "start_line": 19,
          "line_range": [
            19,
            19
          ],
          "target_line_range": [
            20,
            23
          ]
        },
        {
          "code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "display_code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 20,
          "line_range": [
            20,
            23
          ]
        },
        {
          "code": "# Call the Gemini API to generate content with the image and prompt\n",
          "display_code": "",
          "annotation": "Call the Gemini API to generate content with the image and prompt",
          "is_comment": true,
          "start_line": 24,
          "line_range": [
            24,
            24
          ],
          "target_line_range": [
            25,
            28
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-1.5-pro\", contents=[cat_image, prompt]\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-1.5-pro\", contents=[cat_image, prompt]\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            28
          ]
        },
        {
          "code": "# Print the response text, which will contain the bounding box coordinates\n",
          "display_code": "",
          "annotation": "Print the response text, which will contain the bounding box coordinates",
          "is_comment": true,
          "start_line": 29,
          "line_range": [
            29,
            29
          ],
          "target_line_range": [
            30,
            31
          ]
        },
        {
          "code": "print(response.text)\n\n",
          "display_code": "print(response.text)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 30,
          "line_range": [
            30,
            31
          ]
        },
        {
          "code": "#\n# Normalize Coordinates\n# The model returns bounding box coordinates in the format [y_min, x_min, y_max, x_max].\n# To convert these normalized coordinates to the pixel coordinates of your original image, follow these steps:\n# 1. Divide each output coordinate by 1000.\n# 2. Multiply the x-coordinates by the original image width.\n# 3. Multiply the y-coordinates by the original image height.\n#\n# Example Calculation (assuming the model returns [200, 300, 700, 800] and the image is 1000x800):\n",
          "display_code": "",
          "annotation": "Normalize Coordinates\nThe model returns bounding box coordinates in the format [y_min, x_min, y_max, x_max].\nTo convert these normalized coordinates to the pixel coordinates of your original image, follow these steps:\n1. Divide each output coordinate by 1000.\n2. Multiply the x-coordinates by the original image width.\n3. Multiply the y-coordinates by the original image height.\n\nExample Calculation (assuming the model returns [200, 300, 700, 800] and the image is 1000x800):",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            40
          ],
          "target_line_range": [
            41,
            44
          ]
        },
        {
          "code": "y_min = (200 / 1000) * 800  # 160\nx_min = (300 / 1000) * 1000  # 300\ny_max = (700 / 1000) * 800  # 560\nx_max = (800 / 1000) * 1000  # 800\n",
          "display_code": "y_min = (200 / 1000) * 800  # 160\nx_min = (300 / 1000) * 1000  # 300\ny_max = (700 / 1000) * 800  # 560\nx_max = (800 / 1000) * 1000  # 800\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 41,
          "line_range": [
            41,
            44
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library, requests, and Pillow",
          "command": "pip install google-genai Pillow requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python object-detection.py",
          "output": "[0.1, 0.2, 0.7, 0.8]"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox"
      ],
      "section_id": "002-basic-images",
      "section_title": "Images"
    },
    {
      "id": "008-image-segmentation",
      "title": "Image segmentation",
      "description": "This example demonstrates how to use the Gemini API to perform image segmentation on a picture of a cat.",
      "order": 8,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API and necessary libraries. Make sure Pillow is installed!\n",
          "display_code": "",
          "annotation": "Import the Gemini API and necessary libraries. Make sure Pillow is installed!",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            12
          ]
        },
        {
          "code": "from google import genai\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport json\nimport base64\n\n",
          "display_code": "from google import genai\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport json\nimport base64\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            12
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            15
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            15
          ]
        },
        {
          "code": "# Define the prompt for image segmentation, focusing on cats\n",
          "display_code": "",
          "annotation": "Define the prompt for image segmentation, focusing on cats",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            23
          ]
        },
        {
          "code": "prompt = \"\"\"\nGive the segmentation masks for the cat in the image.\nOutput a JSON list of segmentation masks where each entry contains the 2D\nbounding box in the key \\\"box_2d\\\", the segmentation mask in key \\\"mask\\\", and\nthe text label in the key \\\"label\\\". Use descriptive labels.\n\"\"\"\n\n",
          "display_code": "prompt = \"\"\"\nGive the segmentation masks for the cat in the image.\nOutput a JSON list of segmentation masks where each entry contains the 2D\nbounding box in the key \\\"box_2d\\\", the segmentation mask in key \\\"mask\\\", and\nthe text label in the key \\\"label\\\". Use descriptive labels.\n\"\"\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            23
          ]
        },
        {
          "code": "# Download the cat image from cataas.com\n",
          "display_code": "",
          "annotation": "Download the cat image from cataas.com",
          "is_comment": true,
          "start_line": 24,
          "line_range": [
            24,
            24
          ],
          "target_line_range": [
            25,
            28
          ]
        },
        {
          "code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "display_code": "image_url = \"https://cataas.com/cat\"\nresponse = requests.get(image_url)\ncat_image = Image.open(BytesIO(response.content))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            28
          ]
        },
        {
          "code": "# Save the original image\n",
          "display_code": "",
          "annotation": "Save the original image",
          "is_comment": true,
          "start_line": 29,
          "line_range": [
            29,
            29
          ],
          "target_line_range": [
            30,
            33
          ]
        },
        {
          "code": "original_filename = f\"cat_original.png\"\ncat_image.save(original_filename)\nprint(f\"Original image saved as: {original_filename}\")\n\n",
          "display_code": "original_filename = f\"cat_original.png\"\ncat_image.save(original_filename)\nprint(f\"Original image saved as: {original_filename}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 30,
          "line_range": [
            30,
            33
          ]
        },
        {
          "code": "# Call the Gemini API to generate content with the image and prompt\n",
          "display_code": "",
          "annotation": "Call the Gemini API to generate content with the image and prompt",
          "is_comment": true,
          "start_line": 34,
          "line_range": [
            34,
            34
          ],
          "target_line_range": [
            35,
            38
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\", contents=[cat_image, prompt]\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.5-pro-exp-03-25\", contents=[cat_image, prompt]\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 35,
          "line_range": [
            35,
            38
          ]
        },
        {
          "code": "# Print the response containing segmentation information.\n",
          "display_code": "",
          "annotation": "Print the response containing segmentation information.",
          "is_comment": true,
          "start_line": 39,
          "line_range": [
            39,
            39
          ],
          "target_line_range": [
            40,
            41
          ]
        },
        {
          "code": "print(response.text)\n\n",
          "display_code": "print(response.text)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 40,
          "line_range": [
            40,
            41
          ]
        },
        {
          "code": "# Display and save the overlaid mask.\n# Extract the JSON part from the response (it might be wrapped in markdown)\n",
          "display_code": "",
          "annotation": "Display and save the overlaid mask.\nExtract the JSON part from the response (it might be wrapped in markdown)",
          "is_comment": true,
          "start_line": 42,
          "line_range": [
            42,
            43
          ],
          "target_line_range": [
            44,
            53
          ]
        },
        {
          "code": "response_text = response.text\nif \"```json\" in response_text:\n    json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\nelif \"[\" in response_text and \"]\" in response_text:\n    start = response_text.find(\"[\")\n    end = response_text.rfind(\"]\") + 1\n    json_str = response_text[start:end]\nelse:\n    json_str = response_text\n\n",
          "display_code": "response_text = response.text\nif \"```json\" in response_text:\n    json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\nelif \"[\" in response_text and \"]\" in response_text:\n    start = response_text.find(\"[\")\n    end = response_text.rfind(\"]\") + 1\n    json_str = response_text[start:end]\nelse:\n    json_str = response_text\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 44,
          "line_range": [
            44,
            53
          ]
        },
        {
          "code": "# Parse JSON data\n",
          "display_code": "",
          "annotation": "Parse JSON data",
          "is_comment": true,
          "start_line": 54,
          "line_range": [
            54,
            54
          ],
          "target_line_range": [
            55,
            56
          ]
        },
        {
          "code": "mask_data = json.loads(json_str)\n\n",
          "display_code": "mask_data = json.loads(json_str)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 55,
          "line_range": [
            55,
            56
          ]
        },
        {
          "code": "# Get the first mask. This assumes a mask was returned.\n",
          "display_code": "",
          "annotation": "Get the first mask. This assumes a mask was returned.",
          "is_comment": true,
          "start_line": 57,
          "line_range": [
            57,
            57
          ],
          "target_line_range": [
            58,
            59
          ]
        },
        {
          "code": "first_mask = mask_data[0]\n\n",
          "display_code": "first_mask = mask_data[0]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 58,
          "line_range": [
            58,
            59
          ]
        },
        {
          "code": "# Extract base64 encoded mask\n",
          "display_code": "",
          "annotation": "Extract base64 encoded mask",
          "is_comment": true,
          "start_line": 60,
          "line_range": [
            60,
            60
          ],
          "target_line_range": [
            61,
            64
          ]
        },
        {
          "code": "mask_base64 = first_mask.get(\"mask\", \"\")\nif \"base64,\" in mask_base64:\n    mask_base64 = mask_base64.split(\"base64,\")[1]\n\n",
          "display_code": "mask_base64 = first_mask.get(\"mask\", \"\")\nif \"base64,\" in mask_base64:\n    mask_base64 = mask_base64.split(\"base64,\")[1]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 61,
          "line_range": [
            61,
            64
          ]
        },
        {
          "code": "# Decode and load the mask image\n",
          "display_code": "",
          "annotation": "Decode and load the mask image",
          "is_comment": true,
          "start_line": 65,
          "line_range": [
            65,
            65
          ],
          "target_line_range": [
            66,
            68
          ]
        },
        {
          "code": "mask_bytes = base64.b64decode(mask_base64)\nmask_image = Image.open(BytesIO(mask_bytes))\n\n",
          "display_code": "mask_bytes = base64.b64decode(mask_base64)\nmask_image = Image.open(BytesIO(mask_bytes))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 66,
          "line_range": [
            66,
            68
          ]
        },
        {
          "code": "# Convert images to RGBA\n",
          "display_code": "",
          "annotation": "Convert images to RGBA",
          "is_comment": true,
          "start_line": 69,
          "line_range": [
            69,
            69
          ],
          "target_line_range": [
            70,
            72
          ]
        },
        {
          "code": "cat_image = cat_image.convert(\"RGBA\")\nmask_image = mask_image.convert(\"L\")  # Convert mask to grayscale\n\n",
          "display_code": "cat_image = cat_image.convert(\"RGBA\")\nmask_image = mask_image.convert(\"L\")  # Convert mask to grayscale\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 70,
          "line_range": [
            70,
            72
          ]
        },
        {
          "code": "# Create a bright colored overlay (bright pink)\n",
          "display_code": "",
          "annotation": "Create a bright colored overlay (bright pink)",
          "is_comment": true,
          "start_line": 73,
          "line_range": [
            73,
            73
          ],
          "target_line_range": [
            74,
            77
          ]
        },
        {
          "code": "overlay = Image.new(\n    \"RGBA\", mask_image.size, (255, 0, 255, 128)\n)  # Bright pink, semi-transparent\n\n",
          "display_code": "overlay = Image.new(\n    \"RGBA\", mask_image.size, (255, 0, 255, 128)\n)  # Bright pink, semi-transparent\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 74,
          "line_range": [
            74,
            77
          ]
        },
        {
          "code": "# Use the mask to determine where to apply the color.\n# We need the mask as an alpha channel\n",
          "display_code": "",
          "annotation": "Use the mask to determine where to apply the color.\nWe need the mask as an alpha channel",
          "is_comment": true,
          "start_line": 78,
          "line_range": [
            78,
            79
          ],
          "target_line_range": [
            80,
            81
          ]
        },
        {
          "code": "overlay.putalpha(mask_image)\n\n",
          "display_code": "overlay.putalpha(mask_image)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 80,
          "line_range": [
            80,
            81
          ]
        },
        {
          "code": "# Resize the overlay to match the original image if needed\n",
          "display_code": "",
          "annotation": "Resize the overlay to match the original image if needed",
          "is_comment": true,
          "start_line": 82,
          "line_range": [
            82,
            82
          ],
          "target_line_range": [
            83,
            85
          ]
        },
        {
          "code": "if overlay.size != cat_image.size:\n    overlay = overlay.resize(cat_image.size)\n\n",
          "display_code": "if overlay.size != cat_image.size:\n    overlay = overlay.resize(cat_image.size)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 83,
          "line_range": [
            83,
            85
          ]
        },
        {
          "code": "# Overlay the colored mask on the original image. Save both images.\n",
          "display_code": "",
          "annotation": "Overlay the colored mask on the original image. Save both images.",
          "is_comment": true,
          "start_line": 86,
          "line_range": [
            86,
            86
          ],
          "target_line_range": [
            87,
            93
          ]
        },
        {
          "code": "result = Image.alpha_composite(cat_image, overlay)\n\nmask_filename = f\"cat_mask.png\"\nmask_image.save(mask_filename)\n\nmerged_filename = f\"cat_with_mask.png\"\nresult.save(merged_filename)\n",
          "display_code": "result = Image.alpha_composite(cat_image, overlay)\n\nmask_filename = f\"cat_mask.png\"\nmask_image.save(mask_filename)\n\nmerged_filename = f\"cat_with_mask.png\"\nresult.save(merged_filename)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 87,
          "line_range": [
            87,
            93
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python cat_segmentation.py",
          "output": "# Expected output (example):\n# [{\"box_2d\": [100, 50, 900, 750], \"mask\": \"base64_encoded_png_data\", \"label\": \"Main Coon Cat\"}, ...]"
        }
      ],
      "image_data": [
        {
          "path": "examples/008-image-segmentation/image-segmentation.png",
          "filename": "image-segmentation.png",
          "caption": "Segmentation"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#image_segmentation"
      ],
      "section_id": "002-basic-images",
      "section_title": "Images"
    },
    {
      "id": "009-audio-q-a",
      "title": "Audio question answering",
      "description": "This example demonstrates how to ask a question about the content of an audio file using the Gemini API.",
      "order": 9,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the necessary libraries\n",
          "display_code": "",
          "annotation": "Import the necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport requests\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport requests\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Replace with your actual API key\n",
          "display_code": "",
          "annotation": "Replace with your actual API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# Define a descriptive User-Agent following Wikimedia's policy\n",
          "display_code": "",
          "annotation": "Define a descriptive User-Agent following Wikimedia's policy",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            15
          ]
        },
        {
          "code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "display_code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            15
          ]
        },
        {
          "code": "# Download the audio file from the URL\n",
          "display_code": "",
          "annotation": "Download the audio file from the URL",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            21
          ]
        },
        {
          "code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "display_code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            21
          ]
        },
        {
          "code": "# Save the content to a variable\n# Note: If your audio file is larger than 20MB, you should use the File API to upload the file first.\n# The File API allows you to upload larger files and then reference them in your requests.\n",
          "display_code": "",
          "annotation": "Save the content to a variable\nNote: If your audio file is larger than 20MB, you should use the File API to upload the file first.\nThe File API allows you to upload larger files and then reference them in your requests.",
          "is_comment": true,
          "start_line": 22,
          "line_range": [
            22,
            24
          ],
          "target_line_range": [
            25,
            26
          ]
        },
        {
          "code": "audio_bytes = response.content\n\n",
          "display_code": "audio_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            26
          ]
        },
        {
          "code": "# You can now pass that audio file along with the prompt to Gemini\n",
          "display_code": "",
          "annotation": "You can now pass that audio file along with the prompt to Gemini",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            27
          ],
          "target_line_range": [
            28,
            39
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"What is the main topic of this audio?\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\nprint(response.text)\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"What is the main topic of this audio?\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\nprint(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 28,
          "line_range": [
            28,
            39
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and requests",
          "command": "pip install google-genai requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python audio-question.py",
          "output": "This audio features a male host and a travel expert, Pete Trabucco."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/audio?lang=python"
      ],
      "section_id": "003-basic-audio",
      "section_title": "Audio"
    },
    {
      "id": "010-audio-transcription",
      "title": "Audio transcription",
      "description": "This example demonstrates how to transcribe an audio file by providing the audio data inline with the request.",
      "order": 10,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the necessary modules\n",
          "display_code": "",
          "annotation": "Import the necessary modules",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            8
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport requests\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            8
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 9,
          "line_range": [
            9,
            9
          ],
          "target_line_range": [
            10,
            11
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 10,
          "line_range": [
            10,
            11
          ]
        },
        {
          "code": "# Define a descriptive User-Agent following Wikimedia's policy\n",
          "display_code": "",
          "annotation": "Define a descriptive User-Agent following Wikimedia's policy",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "display_code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Download the audio file from the URL\n",
          "display_code": "",
          "annotation": "Download the audio file from the URL",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            20
          ]
        },
        {
          "code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "display_code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            20
          ]
        },
        {
          "code": "# Read the audio file as bytes\n# Note: If your audio file is larger than 20MB, you should use the File API to upload the file first.\n# The File API allows you to upload larger files and then reference them in your requests.\n",
          "display_code": "",
          "annotation": "Read the audio file as bytes\nNote: If your audio file is larger than 20MB, you should use the File API to upload the file first.\nThe File API allows you to upload larger files and then reference them in your requests.",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            23
          ],
          "target_line_range": [
            24,
            25
          ]
        },
        {
          "code": "audio_bytes = response.content\n\n",
          "display_code": "audio_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 24,
          "line_range": [
            24,
            25
          ]
        },
        {
          "code": "# Call the API to generate a transcription of the audio clip\n",
          "display_code": "",
          "annotation": "Call the API to generate a transcription of the audio clip",
          "is_comment": true,
          "start_line": 26,
          "line_range": [
            26,
            26
          ],
          "target_line_range": [
            27,
            37
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"Transcribe this audio clip\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"Transcribe this audio clip\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 27,
          "line_range": [
            27,
            37
          ]
        },
        {
          "code": "# Print the transcribed text\n",
          "display_code": "",
          "annotation": "Print the transcribed text",
          "is_comment": true,
          "start_line": 38,
          "line_range": [
            38,
            38
          ],
          "target_line_range": [
            39,
            39
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 39,
          "line_range": [
            39,
            39
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python audio-transcription.py",
          "output": "We're joined once again by our travel expert and also author of America's Top Roller Coasters and Amusement Parks, Pete Trabucco. Good morning and welcome back to Daybreak USA. Well, thanks for having me on. If someone's lucky enough to go on vacation to an exotic location, and then maybe not so lucky to have some kind of a disaster happen while they're there, maybe some civil unrest. What should they do now? What's the next step? Well, whenever you're going on vacation whether it's locally or internationally, you've got to be uh very careful."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/audio?lang=python"
      ],
      "section_id": "003-basic-audio",
      "section_title": "Audio"
    },
    {
      "id": "011-audio-summarization",
      "title": "Audio summarization",
      "description": "This example demonstrates how to summarize the content of an audio file using the Gemini API.",
      "order": 11,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the necessary libraries\n",
          "display_code": "",
          "annotation": "Import the necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport requests\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport requests\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Replace with your actual API key\n",
          "display_code": "",
          "annotation": "Replace with your actual API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# Define a descriptive User-Agent following Wikimedia's policy\n",
          "display_code": "",
          "annotation": "Define a descriptive User-Agent following Wikimedia's policy",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            15
          ]
        },
        {
          "code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "display_code": "user_agent = \"GeminiByExample/1.0 (https://github.com/strickvl/geminibyexample; contact@example.org) python-requests/2.0\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            15
          ]
        },
        {
          "code": "# Download the audio file from the URL\n",
          "display_code": "",
          "annotation": "Download the audio file from the URL",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            21
          ]
        },
        {
          "code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "display_code": "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/%22DayBreak%22_with_Jay_Young_on_the_USA_Radio_Network.ogg\"\nheaders = {\"User-Agent\": user_agent}\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            21
          ]
        },
        {
          "code": "# Save the content to a variable\n# Note: If your audio file is larger than 20MB, you should use the File API to upload the file first.\n# The File API allows you to upload larger files and then reference them in your requests.\n",
          "display_code": "",
          "annotation": "Save the content to a variable\nNote: If your audio file is larger than 20MB, you should use the File API to upload the file first.\nThe File API allows you to upload larger files and then reference them in your requests.",
          "is_comment": true,
          "start_line": 22,
          "line_range": [
            22,
            24
          ],
          "target_line_range": [
            25,
            26
          ]
        },
        {
          "code": "audio_bytes = response.content\n\n",
          "display_code": "audio_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            26
          ]
        },
        {
          "code": "# You can now pass that audio file along with the prompt to Gemini\n",
          "display_code": "",
          "annotation": "You can now pass that audio file along with the prompt to Gemini",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            27
          ],
          "target_line_range": [
            28,
            39
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"What is this audio about?\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\nprint(response.text)\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"What is this audio about?\",\n        types.Part.from_bytes(\n            data=audio_bytes,\n            mime_type=\"audio/ogg\",\n        ),\n    ],\n)\n\nprint(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 28,
          "line_range": [
            28,
            39
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and requests",
          "command": "pip install google-genai requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python audio-summarization.py",
          "output": "This audio is about travel tips, particularly what to do in the event of a disaster while on vacation.\nThe speaker emphasizes the importance of staying informed about the destination, traveling with a buddy,\nhaving a plan in place, and investing in travel insurance. They also mention the importance of connecting\nwith home base and knowing the location of the American Red Cross in case of emergencies."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/audio?lang=python"
      ],
      "section_id": "003-basic-audio",
      "section_title": "Audio"
    },
    {
      "id": "012-video-q-a",
      "title": "Video question answering",
      "description": "This example demonstrates how to ask questions about a video using the Gemini API.\nNote: For videos larger than 20MB, you must use the File API for uploading.",
      "order": 12,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 4,
          "line_range": [
            4,
            4
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 5,
          "line_range": [
            5,
            5
          ],
          "target_line_range": [
            6,
            10
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            10
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            11
          ],
          "target_line_range": [
            12,
            15
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 12,
          "line_range": [
            12,
            15
          ]
        },
        {
          "code": "# Download the video file.\n# Read the video file as bytes for inline upload.\n",
          "display_code": "",
          "annotation": "Download the video file.\nRead the video file as bytes for inline upload.",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            17
          ],
          "target_line_range": [
            18,
            20
          ]
        },
        {
          "code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "display_code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            20
          ]
        },
        {
          "code": "# Create a Gemini request with the video and a question\n",
          "display_code": "",
          "annotation": "Create a Gemini request with the video and a question",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            31
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=\"Describe the tone and genre of this video.\"),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=\"Describe the tone and genre of this video.\"),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            31
          ]
        },
        {
          "code": "# Print the model's response\n",
          "display_code": "",
          "annotation": "Print the model's response",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            32
          ],
          "target_line_range": [
            33,
            33
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            33
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python video_question_answering.py",
          "output": "Certainly! Here's a description of the tone and genre of the video clip:\n**Genre:**  Travel or scenery/ambient video\n**Tone:** Relaxed, peaceful, and observational. The video presents a serene view of a park next to a busy street. The presence of nature with the sounds of the city creates a tranquil atmosphere."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-video"
      ],
      "section_id": "004-basic-video",
      "section_title": "Video"
    },
    {
      "id": "013-video-summarization",
      "title": "Video summarization",
      "description": "This example demonstrates how to summarize the content of a video using the Gemini API.\nNote: For videos larger than 20MB, you must use the File API for uploading.",
      "order": 13,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 4,
          "line_range": [
            4,
            4
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 5,
          "line_range": [
            5,
            5
          ],
          "target_line_range": [
            6,
            10
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            10
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            11
          ],
          "target_line_range": [
            12,
            15
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 12,
          "line_range": [
            12,
            15
          ]
        },
        {
          "code": "# Download the video file.\n# Read the video file as bytes for inline upload.\n",
          "display_code": "",
          "annotation": "Download the video file.\nRead the video file as bytes for inline upload.",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            17
          ],
          "target_line_range": [
            18,
            20
          ]
        },
        {
          "code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "display_code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            20
          ]
        },
        {
          "code": "# Create a Gemini request with the video and a question\n",
          "display_code": "",
          "annotation": "Create a Gemini request with the video and a question",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            31
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=\"Summarize the content of this video.\"),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=\"Summarize the content of this video.\"),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            31
          ]
        },
        {
          "code": "# Print the model's response\n",
          "display_code": "",
          "annotation": "Print the model's response",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            32
          ],
          "target_line_range": [
            33,
            33
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            33
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python video_summarization.py",
          "output": "The video shows a park with trees next to a busy street with cars and buses passing by. The sun shines through the leaves of the trees."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-video"
      ],
      "section_id": "004-basic-video",
      "section_title": "Video"
    },
    {
      "id": "014-video-transcription",
      "title": "Video transcription",
      "description": "This example demonstrates how to transcribe the content of a video using the Gemini API.\nNote: For videos larger than 20MB, you must use the File API for uploading.",
      "order": 14,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 4,
          "line_range": [
            4,
            4
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 5,
          "line_range": [
            5,
            5
          ],
          "target_line_range": [
            6,
            10
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\nimport requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            10
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            11
          ],
          "target_line_range": [
            12,
            15
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nvideo_url = \"https://download.samplelib.com/mp4/sample-5s.mp4\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 12,
          "line_range": [
            12,
            15
          ]
        },
        {
          "code": "# Download the video file.\n# Read the video file as bytes for inline upload.\n",
          "display_code": "",
          "annotation": "Download the video file.\nRead the video file as bytes for inline upload.",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            17
          ],
          "target_line_range": [
            18,
            20
          ]
        },
        {
          "code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "display_code": "response = requests.get(video_url)\nvideo_bytes = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            20
          ]
        },
        {
          "code": "# Define our prompt\n",
          "display_code": "",
          "annotation": "Define our prompt",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            26
          ]
        },
        {
          "code": "prompt = (\n    \"Transcribe the audio from this video, giving timestamps for \"\n    \"salient events in the video. Also provide visual descriptions.\"\n)\n\n",
          "display_code": "prompt = (\n    \"Transcribe the audio from this video, giving timestamps for \"\n    \"salient events in the video. Also provide visual descriptions.\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            26
          ]
        },
        {
          "code": "# Create a Gemini request with the video and our prompt.\n",
          "display_code": "",
          "annotation": "Create a Gemini request with the video and our prompt.",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            27
          ],
          "target_line_range": [
            28,
            37
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=prompt),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=types.Content(\n        parts=[\n            types.Part(text=prompt),\n            types.Part(inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")),\n        ]\n    ),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 28,
          "line_range": [
            28,
            37
          ]
        },
        {
          "code": "# Print the model's response\n",
          "display_code": "",
          "annotation": "Print the model's response",
          "is_comment": true,
          "start_line": 38,
          "line_range": [
            38,
            38
          ],
          "target_line_range": [
            39,
            39
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 39,
          "line_range": [
            39,
            39
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python video_transcription.py",
          "output": "Okay, here's the transcription and visual descriptions of the video:\n**Video Description:**\nThe video pans up from a low angle showing a park with lush green trees.  Sunlight filters through the leaves. In the distance, cars and a bus can be seen on a road next to the park. There is a paved walkway and low bushes.\n**Timestamps:**\n*   **0:00** Camera starts panning up showing a park with trees and sunlight. \n*   **0:04** The camera reaches its highest point in its view."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-video"
      ],
      "section_id": "004-basic-video",
      "section_title": "Video"
    },
    {
      "id": "015-youtube-video-summarization",
      "title": "YouTube video summarization",
      "description": "This example demonstrates how to summarize a YouTube video using its URL.",
      "order": 15,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            6
          ]
        },
        {
          "code": "from google import genai\n\n",
          "display_code": "from google import genai\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            6
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            7
          ],
          "target_line_range": [
            8,
            9
          ]
        },
        {
          "code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "display_code": "client = genai.Client(api_key=\"YOUR_API_KEY\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 8,
          "line_range": [
            8,
            9
          ]
        },
        {
          "code": "# Construct the prompt with the YouTube video URL\n",
          "display_code": "",
          "annotation": "Construct the prompt with the YouTube video URL",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            13
          ]
        },
        {
          "code": "youtube_url = \"https://www.youtube.com/watch?v=tAP1eZYEuKA\"\nprompt = f\"Summarize the content of this YouTube video: {youtube_url}\"\n\n",
          "display_code": "youtube_url = \"https://www.youtube.com/watch?v=tAP1eZYEuKA\"\nprompt = f\"Summarize the content of this YouTube video: {youtube_url}\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            13
          ]
        },
        {
          "code": "# Call the API to generate content\n",
          "display_code": "",
          "annotation": "Call the API to generate content",
          "is_comment": true,
          "start_line": 14,
          "line_range": [
            14,
            14
          ],
          "target_line_range": [
            15,
            26
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        {\n            \"parts\": [\n                {\"text\": \"Can you summarize this video?\"},\n                {\"file_data\": {\"file_uri\": youtube_url}},\n            ]\n        }\n    ],\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        {\n            \"parts\": [\n                {\"text\": \"Can you summarize this video?\"},\n                {\"file_data\": {\"file_uri\": youtube_url}},\n            ]\n        }\n    ],\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            26
          ]
        },
        {
          "code": "# Print the generated summary\n",
          "display_code": "",
          "annotation": "Print the generated summary",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            27
          ],
          "target_line_range": [
            28,
            28
          ]
        },
        {
          "code": "print(response.text)\n",
          "display_code": "print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 28,
          "line_range": [
            28,
            28
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python youtube-summarization.py",
          "output": "Sure, here is a summary of the video!\nThomas, the father, is sharing his son Max's story of having Alexander Disease, a rare ultra-rare genetic disorder. After having a difficult time conceiving and finally being successful and welcoming Max to their family, they were dealt a devastating blow when Max had his first seizure at a very young age. \nBecause of the seizure, Max had to go through a series of medical tests. Those tests showed that Max had Alexander Disease. After doing some research, the family was heartbroken, as the typical life expectancy for this disease is 5-10 years, and there is no treatment or cure.\nThomas started researching more in-depth by summarizing scientific papers by using Gemini AI and has discovered a lead scientist and her team in New York that he connected with. He sends one to two emails a week to different scientists in order to get more studies underway for the disease. He doesn't want Max to be seen as having 'zero' chance and wants to be a dad and enjoy his time with Max. He will continue to strive to find a cure for Max!"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/vision?lang=python#youtube"
      ],
      "section_id": "004-basic-video",
      "section_title": "Video"
    },
    {
      "id": "016-pdf-csv-analysis",
      "title": "PDF and CSV data analysis and summarization",
      "description": "This example demonstrates how to use the Gemini API to analyze data from PDF and CSV files.",
      "order": 16,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries\n",
          "display_code": "",
          "annotation": "Import necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            9
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport httpx\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport httpx\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            9
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            12
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            12
          ]
        },
        {
          "code": "# We start with the PDF analysis.\n# Download the PDF file\n",
          "display_code": "",
          "annotation": "We start with the PDF analysis.\nDownload the PDF file",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            14
          ],
          "target_line_range": [
            15,
            17
          ]
        },
        {
          "code": "pdf_url = \"https://www.princexml.com/samples/invoice/invoicesample.pdf\"\npdf_data = httpx.get(pdf_url).content\n\n",
          "display_code": "pdf_url = \"https://www.princexml.com/samples/invoice/invoicesample.pdf\"\npdf_data = httpx.get(pdf_url).content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            17
          ]
        },
        {
          "code": "# Prompt to extract main players from the PDF\n",
          "display_code": "",
          "annotation": "Prompt to extract main players from the PDF",
          "is_comment": true,
          "start_line": 18,
          "line_range": [
            18,
            18
          ],
          "target_line_range": [
            19,
            23
          ]
        },
        {
          "code": "pdf_prompt = (\n    \"Identify the main companies or entities mentioned in this invoice. \"\n    \"Summarize the data.\"\n)\n\n",
          "display_code": "pdf_prompt = (\n    \"Identify the main companies or entities mentioned in this invoice. \"\n    \"Summarize the data.\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 19,
          "line_range": [
            19,
            23
          ]
        },
        {
          "code": "# Generate content with the PDF and the prompt\n",
          "display_code": "",
          "annotation": "Generate content with the PDF and the prompt",
          "is_comment": true,
          "start_line": 24,
          "line_range": [
            24,
            24
          ],
          "target_line_range": [
            25,
            32
          ]
        },
        {
          "code": "pdf_response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        types.Part.from_bytes(data=pdf_data, mime_type=\"application/pdf\"),\n        pdf_prompt,\n    ],\n)\n\n",
          "display_code": "pdf_response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        types.Part.from_bytes(data=pdf_data, mime_type=\"application/pdf\"),\n        pdf_prompt,\n    ],\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            32
          ]
        },
        {
          "code": "# Print the PDF analysis result\n",
          "display_code": "",
          "annotation": "Print the PDF analysis result",
          "is_comment": true,
          "start_line": 33,
          "line_range": [
            33,
            33
          ],
          "target_line_range": [
            34,
            35
          ]
        },
        {
          "code": "print(\"PDF Analysis Result:\\n\", pdf_response.text)\n\n",
          "display_code": "print(\"PDF Analysis Result:\\n\", pdf_response.text)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 34,
          "line_range": [
            34,
            35
          ]
        },
        {
          "code": "# Moving on to the CSV analysis now. You'll note that the process is very\n# similar.\n# You can also pass in code files, XML, RTF, Markdown, and more.\n# We download the CSV file here.\n",
          "display_code": "",
          "annotation": "Moving on to the CSV analysis now. You'll note that the process is very\nsimilar.\nYou can also pass in code files, XML, RTF, Markdown, and more.\nWe download the CSV file here.",
          "is_comment": true,
          "start_line": 36,
          "line_range": [
            36,
            39
          ],
          "target_line_range": [
            40,
            42
          ]
        },
        {
          "code": "csv_url = \"https://gist.githubusercontent.com/suellenstringer-hye/f2231b3383538bcb1a5b051c7908f5b7/raw/0f4e0733a434733cda8e749bbbf33a93c2b5bbde/test.csv\"\ncsv_data = httpx.get(csv_url).content\n\n",
          "display_code": "csv_url = \"https://gist.githubusercontent.com/suellenstringer-hye/f2231b3383538bcb1a5b051c7908f5b7/raw/0f4e0733a434733cda8e749bbbf33a93c2b5bbde/test.csv\"\ncsv_data = httpx.get(csv_url).content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 40,
          "line_range": [
            40,
            42
          ]
        },
        {
          "code": "# Prompt to analyze the CSV data\n",
          "display_code": "",
          "annotation": "Prompt to analyze the CSV data",
          "is_comment": true,
          "start_line": 43,
          "line_range": [
            43,
            43
          ],
          "target_line_range": [
            44,
            45
          ]
        },
        {
          "code": "csv_prompt = \"Analyze this data and tell me about the contents. Summarize the data.\"\n\n",
          "display_code": "csv_prompt = \"Analyze this data and tell me about the contents. Summarize the data.\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 44,
          "line_range": [
            44,
            45
          ]
        },
        {
          "code": "# Generate content with the CSV data and the prompt\n",
          "display_code": "",
          "annotation": "Generate content with the CSV data and the prompt",
          "is_comment": true,
          "start_line": 46,
          "line_range": [
            46,
            46
          ],
          "target_line_range": [
            47,
            57
          ]
        },
        {
          "code": "csv_response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        types.Part.from_bytes(\n            data=csv_data,\n            mime_type=\"text/csv\",\n        ),\n        csv_prompt,\n    ],\n)\n\n",
          "display_code": "csv_response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        types.Part.from_bytes(\n            data=csv_data,\n            mime_type=\"text/csv\",\n        ),\n        csv_prompt,\n    ],\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 47,
          "line_range": [
            47,
            57
          ]
        },
        {
          "code": "# Print the CSV analysis result\n",
          "display_code": "",
          "annotation": "Print the CSV analysis result",
          "is_comment": true,
          "start_line": 58,
          "line_range": [
            58,
            58
          ],
          "target_line_range": [
            59,
            59
          ]
        },
        {
          "code": "print(\"\\nCSV Analysis Result:\\n\", csv_response.text)\n",
          "display_code": "print(\"\\nCSV Analysis Result:\\n\", csv_response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 59,
          "line_range": [
            59,
            59
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and httpx (for downloading files)",
          "command": "pip install google-genai httpx pandas",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python pdf_csv_analysis.py",
          "output": "PDF Analysis Result:\n The main company mentioned in the invoice is Sunny Farm.\nCSV Analysis Result:\n Okay, I've analyzed the provided data. Here's a summary of its contents:\n**Data Format:**\n*   The data appears to be in CSV (Comma Separated Values) format.\n*   The first line is a header row defining the fields.\n*   Each subsequent line represents a record containing information about a person.\n**Fields Present:**\nThe data includes the following fields for each person:\n1.  **first\\_name:** The person's first name.\n2.  **last\\_name:** The person's last name.\n3.  **company\\_name:** The name of the company they are associated with.\n4.  **address:** The street address.\n5.  **city:** The city.\n6.  **county:** The county.\n7.  **state:** The state.\n8.  **zip:** The zip code.\n9.  **phone1:** The primary phone number.\n10. **phone2:** A secondary phone number.\n11. **email:** The email address.\n12. **web:** The website address (presumably for the associated company)."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
        "https://ai.google.dev/gemini-api/docs/document-processing?lang=rest"
      ],
      "section_id": "005-other-data-types",
      "section_title": "PDFs and other data types"
    },
    {
      "id": "017-content-translation",
      "title": "Translate documents",
      "description": "This example demonstrates how to load content from a URL and translate it into\nChinese using the Gemini API.\nIt's easy to do the same using PDF or Markdown files, though you might want to\nsplit it up into smaller chunks for better accuracy if your document is long.",
      "order": 17,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            6
          ]
        },
        {
          "code": "# Import the necessary libraries\n",
          "display_code": "",
          "annotation": "Import the necessary libraries",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            7
          ],
          "target_line_range": [
            8,
            11
          ]
        },
        {
          "code": "from google import genai\nimport requests\nimport os\n\n",
          "display_code": "from google import genai\nimport requests\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 8,
          "line_range": [
            8,
            11
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Define the URL of the content to be translated\n",
          "display_code": "",
          "annotation": "Define the URL of the content to be translated",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            17
          ]
        },
        {
          "code": "url = \"https://raw.githubusercontent.com/zenml-io/zenml/refs/heads/main/README.md\"\n\n",
          "display_code": "url = \"https://raw.githubusercontent.com/zenml-io/zenml/refs/heads/main/README.md\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            17
          ]
        },
        {
          "code": "# Fetch the content from the URL\n",
          "display_code": "",
          "annotation": "Fetch the content from the URL",
          "is_comment": true,
          "start_line": 18,
          "line_range": [
            18,
            18
          ],
          "target_line_range": [
            19,
            21
          ]
        },
        {
          "code": "response = requests.get(url)\ntext_content = response.text\n\n",
          "display_code": "response = requests.get(url)\ntext_content = response.text\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 19,
          "line_range": [
            19,
            21
          ]
        },
        {
          "code": "# Define the prompt for translation\n",
          "display_code": "",
          "annotation": "Define the prompt for translation",
          "is_comment": true,
          "start_line": 22,
          "line_range": [
            22,
            22
          ],
          "target_line_range": [
            23,
            24
          ]
        },
        {
          "code": "prompt = f\"Translate the following English text to Chinese: {text_content}\"\n\n",
          "display_code": "prompt = f\"Translate the following English text to Chinese: {text_content}\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 23,
          "line_range": [
            23,
            24
          ]
        },
        {
          "code": "# Generate the translated content using the Gemini API.\n# We're using the 2.0-flash-lite model here for speed, but you probably would\n# want to use a more powerful model for better results.\n",
          "display_code": "",
          "annotation": "Generate the translated content using the Gemini API.\nWe're using the 2.0-flash-lite model here for speed, but you probably would\nwant to use a more powerful model for better results.",
          "is_comment": true,
          "start_line": 25,
          "line_range": [
            25,
            27
          ],
          "target_line_range": [
            28,
            32
          ]
        },
        {
          "code": "model = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=prompt,\n)\n\n",
          "display_code": "model = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=prompt,\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 28,
          "line_range": [
            28,
            32
          ]
        },
        {
          "code": "# Print the translated text\n",
          "display_code": "",
          "annotation": "Print the translated text",
          "is_comment": true,
          "start_line": 33,
          "line_range": [
            33,
            33
          ],
          "target_line_range": [
            34,
            34
          ]
        },
        {
          "code": "print(model.text)\n",
          "display_code": "print(model.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 34,
          "line_range": [
            34,
            34
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and requests",
          "command": "pip install google-genai requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python translate.py",
          "output": "```chinese\n<div align=\"center\">\n  <img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=0fcbab94-8fbe-4a38-93e8-c2348450a42e\" />\n  <h1 align=\"center\">\u8d85\u8d8a\u6f14\u793a\uff1a\u751f\u4ea7\u7ea7 AI \u7cfb\u7edf</h1>\n  <h3 align=\"center\">ZenML \u5c06\u7ecf\u8fc7\u5b9e\u6218\u68c0\u9a8c\u7684 MLOps \u5b9e\u8df5\u5e26\u5165\u60a8\u7684 AI \u5e94\u7528\uff0c\u5904\u7406\u5927\u89c4\u6a21\u7684\u8bc4\u4f30\u3001\u76d1\u63a7\u548c\u90e8\u7f72</h3>\n</div>\n<!-- \u9879\u76ee\u5fbd\u7ae0 -->\n<!--\n*** \u6211\u4f7f\u7528 Markdown \"\u5f15\u7528\u6837\u5f0f\" \u94fe\u63a5\u4ee5\u63d0\u9ad8\u53ef\u8bfb\u6027\u3002\n*** \u5f15\u7528\u94fe\u63a5\u7528\u65b9\u62ec\u53f7 [ ] \u62ec\u8d77\u6765\uff0c\u800c\u4e0d\u662f\u7528\u62ec\u53f7 ( )\u3002\n*** \u8bf7\u53c2\u9605\u672c\u6587\u6863\u5e95\u90e8\uff0c\u4e86\u89e3\u8d21\u732e\u8005\u7f51\u5740\u3001\u5206\u652f\u7f51\u5740\u7b49\u7684\u5f15\u7528\u53d8\u91cf\u58f0\u660e\u3002 \u8fd9\u662f\u4e00\u4e2a\u53ef\u9009\u7684\u3001\u7b80\u6d01\u7684\u8bed\u6cd5\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u3002\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n<div align=\"center\">\n  <!-- \u9879\u76ee Logo -->\n  <br />\n    <a href=\"https://zenml.io\">\n      <img alt=\"ZenML Logo\" src=\"docs/book/.gitbook/assets/header.png\" alt=\"ZenML Logo\">\n    </a>\n  <br />\netc..."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/document-processing?lang=python",
        "https://ai.google.dev/gemini-api/docs/document-processing?lang=rest"
      ],
      "section_id": "005-other-data-types",
      "section_title": "PDFs and other data types"
    },
    {
      "id": "018-structured-data-extraction",
      "title": "Extract structured data from a PDF",
      "description": "This example demonstrates how to extract structured data from a PDF invoice using the Gemini API and Pydantic.",
      "order": 18,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries. You'll need Pydantic for this one.\n",
          "display_code": "",
          "annotation": "Import necessary libraries. You'll need Pydantic for this one.",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            14
          ]
        },
        {
          "code": "import os\nimport requests\nimport json\nimport re\nfrom pydantic import BaseModel, Field\nfrom typing import List, Union\nfrom google import genai\nfrom google.genai import types\n\n\n",
          "display_code": "import os\nimport requests\nimport json\nimport re\nfrom pydantic import BaseModel, Field\nfrom typing import List, Union\nfrom google import genai\nfrom google.genai import types\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            14
          ]
        },
        {
          "code": "# Define Pydantic models for structured data\n",
          "display_code": "",
          "annotation": "Define Pydantic models for structured data",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            31
          ]
        },
        {
          "code": "class Item(BaseModel):\n    name: str\n    price_per_kg: Union[float, str] = Field(..., alias=\"price/kg\")\n    quantity_kg: Union[float, int] = Field(..., alias=\"quantity (kg)\")\n\n\nclass InvoiceContents(BaseModel):\n    sender: str\n    recipient: str\n    address: str\n    full_total: Union[float, str]\n    subtotal: Union[float, str]\n    gst_value: Union[float, str] = Field(..., alias=\"GST\")\n    items: List[Item]\n\n\n",
          "display_code": "class Item(BaseModel):\n    name: str\n    price_per_kg: Union[float, str] = Field(..., alias=\"price/kg\")\n    quantity_kg: Union[float, int] = Field(..., alias=\"quantity (kg)\")\n\n\nclass InvoiceContents(BaseModel):\n    sender: str\n    recipient: str\n    address: str\n    full_total: Union[float, str]\n    subtotal: Union[float, str]\n    gst_value: Union[float, str] = Field(..., alias=\"GST\")\n    items: List[Item]\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            31
          ]
        },
        {
          "code": "# Load the PDF invoice inline from a URL.\n# For PDFs larger than 20MB, you'll need to use the Files API for uploading\n",
          "display_code": "",
          "annotation": "Load the PDF invoice inline from a URL.\nFor PDFs larger than 20MB, you'll need to use the Files API for uploading",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            33
          ],
          "target_line_range": [
            34,
            38
          ]
        },
        {
          "code": "pdf_url = \"https://www.princexml.com/samples/invoice/invoicesample.pdf\"\nresponse = requests.get(pdf_url)\nresponse.raise_for_status()  # Ensure the download was successful\npdf_data = response.content\n\n",
          "display_code": "pdf_url = \"https://www.princexml.com/samples/invoice/invoicesample.pdf\"\nresponse = requests.get(pdf_url)\nresponse.raise_for_status()  # Ensure the download was successful\npdf_data = response.content\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 34,
          "line_range": [
            34,
            38
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 39,
          "line_range": [
            39,
            39
          ],
          "target_line_range": [
            40,
            41
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 40,
          "line_range": [
            40,
            41
          ]
        },
        {
          "code": "# Configure the model for structured output.\n# Specify the prompt text.\n",
          "display_code": "",
          "annotation": "Configure the model for structured output.\nSpecify the prompt text.",
          "is_comment": true,
          "start_line": 42,
          "line_range": [
            42,
            43
          ],
          "target_line_range": [
            44,
            50
          ]
        },
        {
          "code": "model = \"gemini-2.5-pro-preview-03-25\"\nprompt_text = (\n    \"Extract the following information from the invoice: \"\n    \"sender, recipient, address, full_total, subtotal, GST, \"\n    \"and a list of items (name, price/kg, quantity (kg)).\"\n)\n\n",
          "display_code": "model = \"gemini-2.5-pro-preview-03-25\"\nprompt_text = (\n    \"Extract the following information from the invoice: \"\n    \"sender, recipient, address, full_total, subtotal, GST, \"\n    \"and a list of items (name, price/kg, quantity (kg)).\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 44,
          "line_range": [
            44,
            50
          ]
        },
        {
          "code": "# Call the Gemini API to extract structured data from the PDF\n",
          "display_code": "",
          "annotation": "Call the Gemini API to extract structured data from the PDF",
          "is_comment": true,
          "start_line": 51,
          "line_range": [
            51,
            51
          ],
          "target_line_range": [
            52,
            60
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=model,\n    contents=[\n        types.Part.from_bytes(data=pdf_data, mime_type=\"application/pdf\"),\n        prompt_text,\n    ],\n    config=genai.types.GenerateContentConfig(temperature=0.0),\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=model,\n    contents=[\n        types.Part.from_bytes(data=pdf_data, mime_type=\"application/pdf\"),\n        prompt_text,\n    ],\n    config=genai.types.GenerateContentConfig(temperature=0.0),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 52,
          "line_range": [
            52,
            60
          ]
        },
        {
          "code": "# Extract JSON from response text which might be formatted as a markdown code block.\n# Check if the response is a markdown code block and extract the JSON content.\n",
          "display_code": "",
          "annotation": "Extract JSON from response text which might be formatted as a markdown code block.\nCheck if the response is a markdown code block and extract the JSON content.",
          "is_comment": true,
          "start_line": 61,
          "line_range": [
            61,
            62
          ],
          "target_line_range": [
            63,
            69
          ]
        },
        {
          "code": "response_text = response.text\njson_match = re.search(r\"```(?:json)?\\n(.*?)```\", response_text, re.DOTALL)\nif json_match:\n    json_str = json_match.group(1).strip()\nelse:\n    json_str = response_text.strip()\n\n",
          "display_code": "response_text = response.text\njson_match = re.search(r\"```(?:json)?\\n(.*?)```\", response_text, re.DOTALL)\nif json_match:\n    json_str = json_match.group(1).strip()\nelse:\n    json_str = response_text.strip()\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 63,
          "line_range": [
            63,
            69
          ]
        },
        {
          "code": "# Parse the JSON response into the Pydantic model\n",
          "display_code": "",
          "annotation": "Parse the JSON response into the Pydantic model",
          "is_comment": true,
          "start_line": 70,
          "line_range": [
            70,
            70
          ],
          "target_line_range": [
            71,
            73
          ]
        },
        {
          "code": "invoice_data = json.loads(json_str)\ninvoice = InvoiceContents(**invoice_data)\n\n",
          "display_code": "invoice_data = json.loads(json_str)\ninvoice = InvoiceContents(**invoice_data)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 71,
          "line_range": [
            71,
            73
          ]
        },
        {
          "code": "# Print the extracted data as a JSON string\n",
          "display_code": "",
          "annotation": "Print the extracted data as a JSON string",
          "is_comment": true,
          "start_line": 74,
          "line_range": [
            74,
            74
          ],
          "target_line_range": [
            75,
            75
          ]
        },
        {
          "code": "print(invoice.model_dump_json(indent=2))\n",
          "display_code": "print(invoice.model_dump_json(indent=2))\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 75,
          "line_range": [
            75,
            75
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "Install the Google Generative AI library and Pydantic",
          "command": "pip install google-genai pydantic requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python structured-data-extraction.py",
          "output": "{\n  \"sender\": \"SUNNY FARM\",\n  \"recipient\": \"Denny Gunawan\",\n  \"address\": \"221 Queen St\\nMelbourne VIC 3000\",\n  \"full_total\": \"$39.60\",\n  \"subtotal\": \"$36.00\",\n  \"gst_value\": \"$3.60\",\n  \"items\": [\n    {\n      \"name\": \"Apple\",\n      \"price_per_kg\": \"$5.00\",\n      \"quantity_kg\": 1\n    },\n    {\n      \"name\": \"Orange\",\n      \"price_per_kg\": \"$1.99\",\n      \"quantity_kg\": 2\n    },\n    {\n      \"name\": \"Watermelon\",\n      \"price_per_kg\": \"$1.69\",\n      \"quantity_kg\": 3\n    },\n    {\n      \"name\": \"Mango\",\n      \"price_per_kg\": \"$9.56\",\n      \"quantity_kg\": 2\n    },\n    {\n      \"name\": \"Peach\",\n      \"price_per_kg\": \"$2.99\",\n      \"quantity_kg\": 1\n    }\n  ]\n}"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/document-processing",
        "https://ai.google.dev/gemini-api/docs/structured-output?lang=python"
      ],
      "section_id": "005-other-data-types",
      "section_title": "PDFs and other data types"
    },
    {
      "id": "021-tool-use-function-calling",
      "title": "Function calling & tool use",
      "description": "This example demonstrates how to use the Gemini API to call external functions.",
      "order": 21,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries\n",
          "display_code": "",
          "annotation": "Import necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            10
          ]
        },
        {
          "code": "import os\nfrom datetime import datetime\nfrom google import genai\nfrom google.genai import types\n\n\n",
          "display_code": "import os\nfrom datetime import datetime\nfrom google import genai\nfrom google.genai import types\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            10
          ]
        },
        {
          "code": "# Define the function to get temperature for a location.\n# In a real application, this would call a weather API service like OpenWeatherMap or WeatherAPI\n",
          "display_code": "",
          "annotation": "Define the function to get temperature for a location.\nIn a real application, this would call a weather API service like OpenWeatherMap or WeatherAPI",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            12
          ],
          "target_line_range": [
            13,
            32
          ]
        },
        {
          "code": "def get_current_temperature(location: str) -> dict:\n    \"\"\"Gets the current temperature for a given location.\n\n    Note: This is a simplified mock implementation. In a real application,\n    this function would make an API call to a weather service provider.\n    \"\"\"\n    sample_temperatures = {\n        \"London\": 16,\n        \"New York\": 23,\n        \"Tokyo\": 28,\n        \"Sydney\": 20,\n        \"Paris\": 18,\n        \"Berlin\": 17,\n        \"Cairo\": 32,\n        \"Moscow\": 10,\n    }\n    temp = sample_temperatures.get(location, 21)\n    return {\"location\": location, \"temperature\": temp, \"unit\": \"Celsius\"}\n\n\n",
          "display_code": "def get_current_temperature(location: str) -> dict:\n    \"\"\"Gets the current temperature for a given location.\n\n    Note: This is a simplified mock implementation. In a real application,\n    this function would make an API call to a weather service provider.\n    \"\"\"\n    sample_temperatures = {\n        \"London\": 16,\n        \"New York\": 23,\n        \"Tokyo\": 28,\n        \"Sydney\": 20,\n        \"Paris\": 18,\n        \"Berlin\": 17,\n        \"Cairo\": 32,\n        \"Moscow\": 10,\n    }\n    temp = sample_temperatures.get(location, 21)\n    return {\"location\": location, \"temperature\": temp, \"unit\": \"Celsius\"}\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            32
          ]
        },
        {
          "code": "# Define the function to check appointment availability.\n# In a real application, this would query a calendar API like Google Calendar or\n# a booking system.\n# For this example, we're using hard-coded busy slots.\n",
          "display_code": "",
          "annotation": "Define the function to check appointment availability.\nIn a real application, this would query a calendar API like Google Calendar or\na booking system.\nFor this example, we're using hard-coded busy slots.",
          "is_comment": true,
          "start_line": 33,
          "line_range": [
            33,
            36
          ],
          "target_line_range": [
            37,
            73
          ]
        },
        {
          "code": "def check_appointment_availability(date: str, time: str) -> dict:\n    \"\"\"Checks if there's availability for an appointment at the given date and time.\"\"\"\n    busy_slots = [\n        {\"date\": \"2024-07-04\", \"times\": [\"14:00\", \"15:00\", \"16:00\"]},\n        {\"date\": \"2024-07-05\", \"times\": [\"09:00\", \"10:00\", \"11:00\"]},\n        {\"date\": \"2024-07-10\", \"times\": [\"13:00\", \"14:00\"]},\n    ]\n\n    try:\n        datetime.strptime(date, \"%Y-%m-%d\")\n    except ValueError:\n        return {\n            \"available\": False,\n            \"error\": \"Invalid date format. Please use YYYY-MM-DD.\",\n        }\n\n    try:\n        datetime.strptime(time, \"%H:%M\")\n    except ValueError:\n        return {\n            \"available\": False,\n            \"error\": \"Invalid time format. Please use HH:MM in 24-hour format.\",\n        }\n\n    for slot in busy_slots:\n        if slot[\"date\"] == date and time in slot[\"times\"]:\n            return {\n                \"available\": False,\n                \"message\": f\"The slot on {date} at {time} is already booked.\",\n            }\n\n    return {\n        \"available\": True,\n        \"message\": f\"The slot on {date} at {time} is available for booking.\",\n    }\n\n\n",
          "display_code": "def check_appointment_availability(date: str, time: str) -> dict:\n    \"\"\"Checks if there's availability for an appointment at the given date and time.\"\"\"\n    busy_slots = [\n        {\"date\": \"2024-07-04\", \"times\": [\"14:00\", \"15:00\", \"16:00\"]},\n        {\"date\": \"2024-07-05\", \"times\": [\"09:00\", \"10:00\", \"11:00\"]},\n        {\"date\": \"2024-07-10\", \"times\": [\"13:00\", \"14:00\"]},\n    ]\n\n    try:\n        datetime.strptime(date, \"%Y-%m-%d\")\n    except ValueError:\n        return {\n            \"available\": False,\n            \"error\": \"Invalid date format. Please use YYYY-MM-DD.\",\n        }\n\n    try:\n        datetime.strptime(time, \"%H:%M\")\n    except ValueError:\n        return {\n            \"available\": False,\n            \"error\": \"Invalid time format. Please use HH:MM in 24-hour format.\",\n        }\n\n    for slot in busy_slots:\n        if slot[\"date\"] == date and time in slot[\"times\"]:\n            return {\n                \"available\": False,\n                \"message\": f\"The slot on {date} at {time} is already booked.\",\n            }\n\n    return {\n        \"available\": True,\n        \"message\": f\"The slot on {date} at {time} is available for booking.\",\n    }\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 37,
          "line_range": [
            37,
            73
          ]
        },
        {
          "code": "# For Example 1, we will call a single function with Gemini.\n",
          "display_code": "",
          "annotation": "For Example 1, we will call a single function with Gemini.",
          "is_comment": true,
          "start_line": 74,
          "line_range": [
            74,
            74
          ],
          "target_line_range": [
            75,
            76
          ]
        },
        {
          "code": "print(\"\\n--- Example 1: Basic Function Calling ---\\n\")\n\n",
          "display_code": "print(\"\\n--- Example 1: Basic Function Calling ---\\n\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 75,
          "line_range": [
            75,
            76
          ]
        },
        {
          "code": "# First, we define the function declaration that will be provided to the model.\n",
          "display_code": "",
          "annotation": "First, we define the function declaration that will be provided to the model.",
          "is_comment": true,
          "start_line": 77,
          "line_range": [
            77,
            77
          ],
          "target_line_range": [
            78,
            92
          ]
        },
        {
          "code": "weather_function = {\n    \"name\": \"get_current_temperature\",\n    \"description\": \"Gets the current temperature for a given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name, e.g. San Francisco\",\n            },\n        },\n        \"required\": [\"location\"],\n    },\n}\n\n",
          "display_code": "weather_function = {\n    \"name\": \"get_current_temperature\",\n    \"description\": \"Gets the current temperature for a given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name, e.g. San Francisco\",\n            },\n        },\n        \"required\": [\"location\"],\n    },\n}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 78,
          "line_range": [
            78,
            92
          ]
        },
        {
          "code": "# Create a client and configure it with the function declaration\n",
          "display_code": "",
          "annotation": "Create a client and configure it with the function declaration",
          "is_comment": true,
          "start_line": 93,
          "line_range": [
            93,
            93
          ],
          "target_line_range": [
            94,
            97
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\ntools = types.Tool(function_declarations=[weather_function])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\ntools = types.Tool(function_declarations=[weather_function])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 94,
          "line_range": [
            94,
            97
          ]
        },
        {
          "code": "# Send a request to Gemini that will likely trigger the function\n",
          "display_code": "",
          "annotation": "Send a request to Gemini that will likely trigger the function",
          "is_comment": true,
          "start_line": 98,
          "line_range": [
            98,
            98
          ],
          "target_line_range": [
            99,
            104
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=\"What's the temperature in London?\",\n    config=config,\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=\"What's the temperature in London?\",\n    config=config,\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 99,
          "line_range": [
            99,
            104
          ]
        },
        {
          "code": "# Check if Gemini responded with a function call\n# Assumes Gemini will always respond with a function call.\n",
          "display_code": "",
          "annotation": "Check if Gemini responded with a function call\nAssumes Gemini will always respond with a function call.",
          "is_comment": true,
          "start_line": 105,
          "line_range": [
            105,
            106
          ],
          "target_line_range": [
            107,
            110
          ]
        },
        {
          "code": "function_call = response.candidates[0].content.parts[0].function_call\nprint(f\"Function to call: {function_call.name}\")\nprint(f\"Arguments: {function_call.args}\")\n\n",
          "display_code": "function_call = response.candidates[0].content.parts[0].function_call\nprint(f\"Function to call: {function_call.name}\")\nprint(f\"Arguments: {function_call.args}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 107,
          "line_range": [
            107,
            110
          ]
        },
        {
          "code": "# Execute the function with the arguments Gemini provided\n",
          "display_code": "",
          "annotation": "Execute the function with the arguments Gemini provided",
          "is_comment": true,
          "start_line": 111,
          "line_range": [
            111,
            111
          ],
          "target_line_range": [
            112,
            114
          ]
        },
        {
          "code": "result = get_current_temperature(**function_call.args)\nprint(f\"Function result: {result}\")\n\n",
          "display_code": "result = get_current_temperature(**function_call.args)\nprint(f\"Function result: {result}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 112,
          "line_range": [
            112,
            114
          ]
        },
        {
          "code": "# Send the function result back to Gemini for a final response\n",
          "display_code": "",
          "annotation": "Send the function result back to Gemini for a final response",
          "is_comment": true,
          "start_line": 115,
          "line_range": [
            115,
            115
          ],
          "target_line_range": [
            116,
            133
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        {\n            \"parts\": [\n                {\n                    \"function_response\": {\n                        \"name\": function_call.name,\n                        \"response\": result,\n                    }\n                }\n            ]\n        }\n    ],\n)\nprint(f\"Model's final response: {response.text}\")\n\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        {\n            \"parts\": [\n                {\n                    \"function_response\": {\n                        \"name\": function_call.name,\n                        \"response\": result,\n                    }\n                }\n            ]\n        }\n    ],\n)\nprint(f\"Model's final response: {response.text}\")\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 116,
          "line_range": [
            116,
            133
          ]
        },
        {
          "code": "# Example 2 shows how to use multiple functions simultaneously.\n",
          "display_code": "",
          "annotation": "Example 2 shows how to use multiple functions simultaneously.",
          "is_comment": true,
          "start_line": 134,
          "line_range": [
            134,
            134
          ],
          "target_line_range": [
            135,
            136
          ]
        },
        {
          "code": "print(\"\\n--- Example 2: Parallel Function Calling (Weather and Appointments) ---\\n\")\n\n",
          "display_code": "print(\"\\n--- Example 2: Parallel Function Calling (Weather and Appointments) ---\\n\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 135,
          "line_range": [
            135,
            136
          ]
        },
        {
          "code": "# Define the weather function declaration\n",
          "display_code": "",
          "annotation": "Define the weather function declaration",
          "is_comment": true,
          "start_line": 137,
          "line_range": [
            137,
            137
          ],
          "target_line_range": [
            138,
            152
          ]
        },
        {
          "code": "weather_function = {\n    \"name\": \"get_current_temperature\",\n    \"description\": \"Gets the current temperature for a given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name, e.g. London\",\n            },\n        },\n        \"required\": [\"location\"],\n    },\n}\n\n",
          "display_code": "weather_function = {\n    \"name\": \"get_current_temperature\",\n    \"description\": \"Gets the current temperature for a given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name, e.g. London\",\n            },\n        },\n        \"required\": [\"location\"],\n    },\n}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 138,
          "line_range": [
            138,
            152
          ]
        },
        {
          "code": "# Define the appointment function declaration\n",
          "display_code": "",
          "annotation": "Define the appointment function declaration",
          "is_comment": true,
          "start_line": 153,
          "line_range": [
            153,
            153
          ],
          "target_line_range": [
            154,
            172
          ]
        },
        {
          "code": "appointment_function = {\n    \"name\": \"check_appointment_availability\",\n    \"description\": \"Checks if there's availability for an appointment at the given date and time.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date to check (YYYY-MM-DD)\",\n            },\n            \"time\": {\n                \"type\": \"string\",\n                \"description\": \"Time to check (HH:MM) in 24-hour format\",\n            },\n        },\n        \"required\": [\"date\", \"time\"],\n    },\n}\n\n",
          "display_code": "appointment_function = {\n    \"name\": \"check_appointment_availability\",\n    \"description\": \"Checks if there's availability for an appointment at the given date and time.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date to check (YYYY-MM-DD)\",\n            },\n            \"time\": {\n                \"type\": \"string\",\n                \"description\": \"Time to check (HH:MM) in 24-hour format\",\n            },\n        },\n        \"required\": [\"date\", \"time\"],\n    },\n}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 154,
          "line_range": [
            154,
            172
          ]
        },
        {
          "code": "# Create a client and configure it with both function declarations\n",
          "display_code": "",
          "annotation": "Create a client and configure it with both function declarations",
          "is_comment": true,
          "start_line": 173,
          "line_range": [
            173,
            173
          ],
          "target_line_range": [
            174,
            176
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\ntools = [types.Tool(function_declarations=[weather_function, appointment_function])]\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\ntools = [types.Tool(function_declarations=[weather_function, appointment_function])]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 174,
          "line_range": [
            174,
            176
          ]
        },
        {
          "code": "# Set a lower temperature for more predictable function calling\n",
          "display_code": "",
          "annotation": "Set a lower temperature for more predictable function calling",
          "is_comment": true,
          "start_line": 177,
          "line_range": [
            177,
            177
          ],
          "target_line_range": [
            178,
            182
          ]
        },
        {
          "code": "config = {\n    \"tools\": tools,\n    \"temperature\": 0.1,\n}\n\n",
          "display_code": "config = {\n    \"tools\": tools,\n    \"temperature\": 0.1,\n}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 178,
          "line_range": [
            178,
            182
          ]
        },
        {
          "code": "# Start a chat and send a message that should trigger both functions\n",
          "display_code": "",
          "annotation": "Start a chat and send a message that should trigger both functions",
          "is_comment": true,
          "start_line": 183,
          "line_range": [
            183,
            183
          ],
          "target_line_range": [
            184,
            188
          ]
        },
        {
          "code": "chat = client.chats.create(model=\"gemini-2.0-flash-lite\", config=config)\nresponse = chat.send_message(\n    \"I'm planning to visit Paris on July 4th at 2 PM. What's the weather like there and is that slot available for an appointment?\"\n)\n\n",
          "display_code": "chat = client.chats.create(model=\"gemini-2.0-flash-lite\", config=config)\nresponse = chat.send_message(\n    \"I'm planning to visit Paris on July 4th at 2 PM. What's the weather like there and is that slot available for an appointment?\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 184,
          "line_range": [
            184,
            188
          ]
        },
        {
          "code": "# Store the results from each function call\n",
          "display_code": "",
          "annotation": "Store the results from each function call",
          "is_comment": true,
          "start_line": 189,
          "line_range": [
            189,
            189
          ],
          "target_line_range": [
            190,
            191
          ]
        },
        {
          "code": "results = {}\n\n",
          "display_code": "results = {}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 190,
          "line_range": [
            190,
            191
          ]
        },
        {
          "code": "# Process each function call Gemini requests\n# Assumes Gemini will always respond with function calls.\n",
          "display_code": "",
          "annotation": "Process each function call Gemini requests\nAssumes Gemini will always respond with function calls.",
          "is_comment": true,
          "start_line": 192,
          "line_range": [
            192,
            193
          ],
          "target_line_range": [
            194,
            197
          ]
        },
        {
          "code": "for fn in response.function_calls:\n    args_str = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n    print(f\"{fn.name}({args_str})\")\n\n",
          "display_code": "for fn in response.function_calls:\n    args_str = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n    print(f\"{fn.name}({args_str})\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 194,
          "line_range": [
            194,
            197
          ]
        },
        {
          "code": "    # Call the appropriate function based on name\n",
          "display_code": "",
          "annotation": "Call the appropriate function based on name",
          "is_comment": true,
          "start_line": 198,
          "line_range": [
            198,
            198
          ],
          "target_line_range": [
            199,
            205
          ]
        },
        {
          "code": "    if fn.name == \"get_current_temperature\":\n        result = get_current_temperature(**fn.args)\n    elif fn.name == \"check_appointment_availability\":\n        result = check_appointment_availability(**fn.args)\n    else:\n        result = {\"error\": f\"Unknown function: {fn.name}\"}\n\n",
          "display_code": "    if fn.name == \"get_current_temperature\":\n        result = get_current_temperature(**fn.args)\n    elif fn.name == \"check_appointment_availability\":\n        result = check_appointment_availability(**fn.args)\n    else:\n        result = {\"error\": f\"Unknown function: {fn.name}\"}\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 199,
          "line_range": [
            199,
            205
          ]
        },
        {
          "code": "    # Store each result for later use\n",
          "display_code": "",
          "annotation": "Store each result for later use",
          "is_comment": true,
          "start_line": 206,
          "line_range": [
            206,
            206
          ],
          "target_line_range": [
            207,
            209
          ]
        },
        {
          "code": "    results[fn.name] = result\n    print(f\"Result: {result}\\n\")\n\n",
          "display_code": "    results[fn.name] = result\n    print(f\"Result: {result}\\n\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 207,
          "line_range": [
            207,
            209
          ]
        },
        {
          "code": "# Prepare all function responses to send back to Gemini\n",
          "display_code": "",
          "annotation": "Prepare all function responses to send back to Gemini",
          "is_comment": true,
          "start_line": 210,
          "line_range": [
            210,
            210
          ],
          "target_line_range": [
            211,
            214
          ]
        },
        {
          "code": "function_responses = []\nfor fn_name, result in results.items():\n    function_responses.append({\"name\": fn_name, \"response\": result})\n\n",
          "display_code": "function_responses = []\nfor fn_name, result in results.items():\n    function_responses.append({\"name\": fn_name, \"response\": result})\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 211,
          "line_range": [
            211,
            214
          ]
        },
        {
          "code": "# Send all results back to Gemini in a single message\n",
          "display_code": "",
          "annotation": "Send all results back to Gemini in a single message",
          "is_comment": true,
          "start_line": 215,
          "line_range": [
            215,
            215
          ],
          "target_line_range": [
            216,
            219
          ]
        },
        {
          "code": "if function_responses:\n    print(\"Sending all function results back to the model...\\n\")\n    response = chat.send_message(str(function_responses))\n    print(f\"Model's final response:\\n{response.text}\")\n",
          "display_code": "if function_responses:\n    print(\"Sending all function results back to the model...\\n\")\n    response = chat.send_message(str(function_responses))\n    print(f\"Model's final response:\\n{response.text}\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 216,
          "line_range": [
            216,
            219
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library and requests",
          "command": "pip install google-genai requests",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python function_calling_weather_calendar.py",
          "output": "--- Example 1: Basic Function Calling ---\nFunction to call: get_current_temperature\nArguments: {'location': 'London'}\nFunction result: {'location': 'London', 'temperature': 16, 'unit': 'Celsius'}\nModel's final response: OK. The current temperature in London is 16 degrees Celsius.\n--- Example 2: Parallel Function Calling (Weather and Appointments) ---\nget_current_temperature(location=Paris)\nResult: {'location': 'Paris', 'temperature': 18, 'unit': 'Celsius'}\ncheck_appointment_availability(time=14:00, date=2024-07-04)\nResult: {'available': False, 'message': 'The slot on 2024-07-04 at 14:00 is already booked.'}\nSending all function results back to the model...\nModel's final response:\nThe current temperature in Paris is 18 degrees Celsius. The appointment slot on July 4th at 2 PM is not available."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/function-calling?example=weather",
        "https://ai.google.dev/gemini-api/docs/function-calling?example=meeting"
      ],
      "section_id": "006-agentic",
      "section_title": "Agentic behaviour"
    },
    {
      "id": "022-code-execution",
      "title": "Code execution",
      "description": "This example demonstrates how to use the Gemini API to execute code\n(agent-style) and calculate the sum of the first 50 prime numbers.",
      "order": 22,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 4,
          "line_range": [
            4,
            4
          ]
        },
        {
          "code": "# Import the necessary libraries. Make sure you have the rich library installed!\n",
          "display_code": "",
          "annotation": "Import the necessary libraries. Make sure you have the rich library installed!",
          "is_comment": true,
          "start_line": 5,
          "line_range": [
            5,
            5
          ],
          "target_line_range": [
            6,
            13
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.syntax import Syntax\nfrom rich.panel import Panel\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.syntax import Syntax\nfrom rich.panel import Panel\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            13
          ]
        },
        {
          "code": "# Initialize the rich console and the Gemini client\n",
          "display_code": "",
          "annotation": "Initialize the rich console and the Gemini client",
          "is_comment": true,
          "start_line": 14,
          "line_range": [
            14,
            14
          ],
          "target_line_range": [
            15,
            17
          ]
        },
        {
          "code": "console = Console()\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "console = Console()\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            17
          ]
        },
        {
          "code": "# Configure the model to use the code execution tool.\n# Note that not all models support code execution.\n# The code execution environment includes a number of popular libraries like\n# sklearn, matplotlib, pandas, pdfminer and so on.\n# You can't install your own libraries.\n",
          "display_code": "",
          "annotation": "Configure the model to use the code execution tool.\nNote that not all models support code execution.\nThe code execution environment includes a number of popular libraries like\nsklearn, matplotlib, pandas, pdfminer and so on.\nYou can't install your own libraries.",
          "is_comment": true,
          "start_line": 18,
          "line_range": [
            18,
            22
          ],
          "target_line_range": [
            23,
            38
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"What is the sum of the first 50 prime numbers? \"\n    \"Generate and run code for the calculation, and make sure you get all 50.\",\n    config=types.GenerateContentConfig(\n        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n    ),\n)\n\n\nfor part in response.candidates[0].content.parts:\n    if part.text is not None:\n        console.print(Markdown(part.text))\n\n    if part.executable_code is not None:\n        code = part.executable_code.code\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"What is the sum of the first 50 prime numbers? \"\n    \"Generate and run code for the calculation, and make sure you get all 50.\",\n    config=types.GenerateContentConfig(\n        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n    ),\n)\n\n\nfor part in response.candidates[0].content.parts:\n    if part.text is not None:\n        console.print(Markdown(part.text))\n\n    if part.executable_code is not None:\n        code = part.executable_code.code\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 23,
          "line_range": [
            23,
            38
          ]
        },
        {
          "code": "        # Detect language (simple approach)\n",
          "display_code": "",
          "annotation": "Detect language (simple approach)",
          "is_comment": true,
          "start_line": 39,
          "line_range": [
            39,
            39
          ],
          "target_line_range": [
            40,
            67
          ]
        },
        {
          "code": "        language = (\n            \"python\"\n            if \"def \" in code or \"import \" in code or \"print(\" in code\n            else \"text\"\n        )\n        console.print(\n            Panel(\n                Syntax(code, language, theme=\"monokai\", line_numbers=True),\n                title=\"Code\",\n                border_style=\"blue\",\n            )\n        )\n\n    if part.code_execution_result is not None:\n        console.print(\n            Panel(\n                part.code_execution_result.output,\n                title=\"Output\",\n                border_style=\"green\",\n            )\n        )\n\n    if part.inline_data is not None:\n        console.print(\n            \"[yellow]Image data available but cannot be displayed in terminal[/yellow]\"\n        )\n\n    console.print(\"---\")\n",
          "display_code": "        language = (\n            \"python\"\n            if \"def \" in code or \"import \" in code or \"print(\" in code\n            else \"text\"\n        )\n        console.print(\n            Panel(\n                Syntax(code, language, theme=\"monokai\", line_numbers=True),\n                title=\"Code\",\n                border_style=\"blue\",\n            )\n        )\n\n    if part.code_execution_result is not None:\n        console.print(\n            Panel(\n                part.code_execution_result.output,\n                title=\"Output\",\n                border_style=\"green\",\n            )\n        )\n\n    if part.inline_data is not None:\n        console.print(\n            \"[yellow]Image data available but cannot be displayed in terminal[/yellow]\"\n        )\n\n    console.print(\"---\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 40,
          "line_range": [
            40,
            67
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai rich",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python code-execution.py",
          "output": "Okay, I can help you find the sum of the first 50 prime numbers. Here's how I'll approach this:                                       \n 1 Generate a list of the first 50 prime numbers. I'll need an efficient way to identify prime numbers. I can use the Sieve of        \n   Eratosthenes method or a simpler trial division approach.                                                                          \n 2 Sum the prime numbers. Once I have the list, I'll simply add them up.                                                              \nHere's the Python code to accomplish this:                                                                                            \n---\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    1 def is_prime(n):                                                                                                              \u2502\n\u2502    2     \"\"\"Efficiently determine if a number is prime.\"\"\"                                                                         \u2502\n\u2502    3     if n <= 1:                                                                                                                \u2502\n\u2502    4         return False                                                                                                          \u2502\n\u2502    5     if n <= 3:                                                                                                                \u2502\n\u2502    6         return True                                                                                                           \u2502\n\u2502    7     if n % 2 == 0 or n % 3 == 0:                                                                                              \u2502\n\u2502    8         return False                                                                                                          \u2502\n\u2502    9     i = 5                                                                                                                     \u2502\n\u2502   10     while i * i <= n:                                                                                                         \u2502\n\u2502   11         if n % i == 0 or n % (i + 2) == 0:                                                                                    \u2502\n\u2502   12             return False                                                                                                      \u2502\n\u2502   13         i += 6                                                                                                                \u2502\n\u2502   14     return True                                                                                                               \u2502\n\u2502   15                                                                                                                               \u2502\n\u2502   16 primes = []                                                                                                                   \u2502\n\u2502   17 num = 2                                                                                                                       \u2502\n\u2502   18 while len(primes) < 50:                                                                                                       \u2502\n\u2502   19     if is_prime(num):                                                                                                         \u2502\n\u2502   20         primes.append(num)                                                                                                    \u2502\n\u2502   21     num += 1                                                                                                                  \u2502\n\u2502   22                                                                                                                               \u2502\n\u2502   23 print(f'{primes=}')                                                                                                           \u2502\n\u2502   24 print(f'{sum(primes)=}')                                                                                                      \u2502\n\u2502   25                                                                                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n---\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113,   \u2502\n\u2502 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]                                \u2502\n\u2502 sum(primes)=5117                                                                                                                   \u2502\n\u2502                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n---\nThe sum of the first 50 prime numbers is 5117.                                                                                        \n---"
        }
      ],
      "image_data": [
        {
          "path": "examples/022-code-execution/code_execution.png",
          "filename": "code_execution.png",
          "caption": "code_execution"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/code-execution?lang=python",
        "https://ai.google.dev/gemini-api/docs/code-execution?lang=rest"
      ],
      "section_id": "006-agentic",
      "section_title": "Agentic behaviour"
    },
    {
      "id": "023-mcp-model-context-protocol",
      "title": "Model Context Protocol",
      "description": "This example demonstrates using a local MCP server with Gemini to get weather information.",
      "order": 23,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import necessary libraries. Make you have mcp installed.\n",
          "display_code": "",
          "annotation": "Import necessary libraries. Make you have mcp installed.",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            12
          ]
        },
        {
          "code": "import asyncio\nimport os\nfrom datetime import datetime\nfrom google import genai\nfrom google.genai import types\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n",
          "display_code": "import asyncio\nimport os\nfrom datetime import datetime\nfrom google import genai\nfrom google.genai import types\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            12
          ]
        },
        {
          "code": "# Configure Gemini client\n",
          "display_code": "",
          "annotation": "Configure Gemini client",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            15
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            15
          ]
        },
        {
          "code": "# Define server parameters for the MCP server\n",
          "display_code": "",
          "annotation": "Define server parameters for the MCP server",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            25
          ]
        },
        {
          "code": "server_params = StdioServerParameters(\n    command=\"npx\",  # Executable for the MCP server\n    args=[\n        \"-y\",\n        \"@philschmid/weather-mcp\",\n    ],  # Arguments for the server (Weather MCP Server)\n    env=None,  # Optional environment variables\n)\n\n",
          "display_code": "server_params = StdioServerParameters(\n    command=\"npx\",  # Executable for the MCP server\n    args=[\n        \"-y\",\n        \"@philschmid/weather-mcp\",\n    ],  # Arguments for the server (Weather MCP Server)\n    env=None,  # Optional environment variables\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            25
          ]
        },
        {
          "code": "# Define the prompt to get the weather for the current day in Delft\n",
          "display_code": "",
          "annotation": "Define the prompt to get the weather for the current day in Delft",
          "is_comment": true,
          "start_line": 26,
          "line_range": [
            26,
            26
          ],
          "target_line_range": [
            27,
            29
          ]
        },
        {
          "code": "PROMPT = f\"What is the weather in Delft in {datetime.now().strftime('%Y-%m-%d')}?\"\n\n\n",
          "display_code": "PROMPT = f\"What is the weather in Delft in {datetime.now().strftime('%Y-%m-%d')}?\"\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 27,
          "line_range": [
            27,
            29
          ]
        },
        {
          "code": "# Define an asynchronous function to run the MCP client and interact with\n# Gemini.\n# We retrieve tools from the MCP session and convert them to Gemini Tool objects\n",
          "display_code": "",
          "annotation": "Define an asynchronous function to run the MCP client and interact with\nGemini.\nWe retrieve tools from the MCP session and convert them to Gemini Tool objects",
          "is_comment": true,
          "start_line": 30,
          "line_range": [
            30,
            32
          ],
          "target_line_range": [
            33,
            78
          ]
        },
        {
          "code": "async def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            mcp_tools = await session.list_tools()\n            tools = [\n                types.Tool(\n                    function_declarations=[\n                        {\n                            \"name\": tool.name,\n                            \"description\": tool.description,\n                            \"parameters\": {\n                                k: v\n                                for k, v in tool.inputSchema.items()\n                                if k not in [\"additionalProperties\", \"$schema\"]\n                            },\n                        }\n                    ]\n                )\n                for tool in mcp_tools.tools\n            ]\n\n            response = client.models.generate_content(\n                model=\"gemini-2.0-flash\",\n                contents=PROMPT,\n                config=types.GenerateContentConfig(\n                    temperature=0,\n                    tools=tools,\n                ),\n            )\n\n            if response.candidates[0].content.parts[0].function_call:\n                function_call = response.candidates[0].content.parts[0].function_call\n                print(f\"Function call: {function_call}\")\n\n                result = await session.call_tool(\n                    function_call.name, arguments=function_call.args\n                )\n                print(f\"Tool Result: {result.content[0].text}\")\n\n            else:\n                print(\"No function call found in the response.\")\n                print(response.text)\n\n\n",
          "display_code": "async def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            mcp_tools = await session.list_tools()\n            tools = [\n                types.Tool(\n                    function_declarations=[\n                        {\n                            \"name\": tool.name,\n                            \"description\": tool.description,\n                            \"parameters\": {\n                                k: v\n                                for k, v in tool.inputSchema.items()\n                                if k not in [\"additionalProperties\", \"$schema\"]\n                            },\n                        }\n                    ]\n                )\n                for tool in mcp_tools.tools\n            ]\n\n            response = client.models.generate_content(\n                model=\"gemini-2.0-flash\",\n                contents=PROMPT,\n                config=types.GenerateContentConfig(\n                    temperature=0,\n                    tools=tools,\n                ),\n            )\n\n            if response.candidates[0].content.parts[0].function_call:\n                function_call = response.candidates[0].content.parts[0].function_call\n                print(f\"Function call: {function_call}\")\n\n                result = await session.call_tool(\n                    function_call.name, arguments=function_call.args\n                )\n                print(f\"Tool Result: {result.content[0].text}\")\n\n            else:\n                print(\"No function call found in the response.\")\n                print(response.text)\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            78
          ]
        },
        {
          "code": "# Run the asynchronous function\n",
          "display_code": "",
          "annotation": "Run the asynchronous function",
          "is_comment": true,
          "start_line": 79,
          "line_range": [
            79,
            79
          ],
          "target_line_range": [
            80,
            80
          ]
        },
        {
          "code": "asyncio.run(run())\n",
          "display_code": "asyncio.run(run())\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 80,
          "line_range": [
            80,
            80
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the necessary libraries",
          "command": "pip install google-genai mcp",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python mcp_example.py",
          "output": "Function call: id=None args={'date': '2025-04-05', 'location': 'Delft'} name='get_weather_forecast'\nTool Result: {\"2025-04-05T00:00\":11.4,\"2025-04-05T01:00\":10.3,\"2025-04-05T02:00\":9.8,\"2025-04-05T03:00\":9.1,\"2025-04-05T04:00\":8,\"2025-04-05T05:00\":8,\"2025-04-05T06:00\":8.3,\"2025-04-05T07:00\":9.1,\"2025-04-05T08:00\":11.1,\"2025-04-05T09:00\":12.8,\"2025-04-05T10:00\":14.3,\"2025-04-05T11:00\":15.6,\"2025-04-05T12:00\":16,\"2025-04-05T13:00\":16.4,\"2025-04-05T14:00\":17,\"2025-04-05T15:00\":16.6,\"2025-04-05T16:00\":16.1,\"2025-04-05T17:00\":15,\"2025-04-05T18:00\":13.5,\"2025-04-05T19:00\":11.9,\"2025-04-05T20:00\":11.1,\"2025-04-05T21:00\":10.7,\"2025-04-05T22:00\":10.1,\"2025-04-05T23:00\":9.3}"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/function-calling?example=weather#use_model_context_protocol_mcp"
      ],
      "section_id": "006-agentic",
      "section_title": "Agentic behaviour"
    },
    {
      "id": "024-grounded-responses",
      "title": "Grounded responses with search tool",
      "description": "This example demonstrates how to use the Gemini API with the Search tool to\nget grounded responses.\nThis means that you can ask questions to the LLM which will incorporate live\nor dynamic search results into the response.",
      "order": 24,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            6
          ]
        },
        {
          "code": "# Import the Gemini API and necessary tools\n",
          "display_code": "",
          "annotation": "Import the Gemini API and necessary tools",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            7
          ],
          "target_line_range": [
            8,
            11
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai.types import Tool, GenerateContentConfig, GoogleSearch\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai.types import Tool, GenerateContentConfig, GoogleSearch\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 8,
          "line_range": [
            8,
            11
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Specify the model to use (Gemini 2.0 models or later support search as a tool)\n",
          "display_code": "",
          "annotation": "Specify the model to use (Gemini 2.0 models or later support search as a tool)",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            17
          ]
        },
        {
          "code": "model_id = \"gemini-2.0-flash\"\n\n",
          "display_code": "model_id = \"gemini-2.0-flash\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            17
          ]
        },
        {
          "code": "# Configure the Google Search tool\n",
          "display_code": "",
          "annotation": "Configure the Google Search tool",
          "is_comment": true,
          "start_line": 18,
          "line_range": [
            18,
            18
          ],
          "target_line_range": [
            19,
            20
          ]
        },
        {
          "code": "google_search_tool = Tool(google_search=GoogleSearch())\n\n",
          "display_code": "google_search_tool = Tool(google_search=GoogleSearch())\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 19,
          "line_range": [
            19,
            20
          ]
        },
        {
          "code": "# Craft your query.  This one asks for films in Delft cinemas on a specific date.\n",
          "display_code": "",
          "annotation": "Craft your query.  This one asks for films in Delft cinemas on a specific date.",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            23
          ]
        },
        {
          "code": "query = \"What films are showing in Delft on April 5, 2025, particularly at Filmhuis Lumen and Pathe cinemas?\"\n\n",
          "display_code": "query = \"What films are showing in Delft on April 5, 2025, particularly at Filmhuis Lumen and Pathe cinemas?\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            23
          ]
        },
        {
          "code": "# Call the API to generate content, including the search tool in the configuration\n",
          "display_code": "",
          "annotation": "Call the API to generate content, including the search tool in the configuration",
          "is_comment": true,
          "start_line": 24,
          "line_range": [
            24,
            24
          ],
          "target_line_range": [
            25,
            33
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=model_id,\n    contents=query,\n    config=GenerateContentConfig(\n        tools=[google_search_tool],\n        response_modalities=[\"TEXT\"],\n    ),\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=model_id,\n    contents=query,\n    config=GenerateContentConfig(\n        tools=[google_search_tool],\n        response_modalities=[\"TEXT\"],\n    ),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            33
          ]
        },
        {
          "code": "# Print the generated text response\n",
          "display_code": "",
          "annotation": "Print the generated text response",
          "is_comment": true,
          "start_line": 34,
          "line_range": [
            34,
            34
          ],
          "target_line_range": [
            35,
            37
          ]
        },
        {
          "code": "for each in response.candidates[0].content.parts:\n    print(each.text)\n\n",
          "display_code": "for each in response.candidates[0].content.parts:\n    print(each.text)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 35,
          "line_range": [
            35,
            37
          ]
        },
        {
          "code": "# (Optional) Print the grounding metadata (web content used to ground the response).\n# You'll get a lot of HTML data and content when you print this.\n",
          "display_code": "",
          "annotation": "(Optional) Print the grounding metadata (web content used to ground the response).\nYou'll get a lot of HTML data and content when you print this.",
          "is_comment": true,
          "start_line": 38,
          "line_range": [
            38,
            39
          ],
          "target_line_range": [
            40,
            40
          ]
        },
        {
          "code": "print(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)\n",
          "display_code": "print(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 40,
          "line_range": [
            40,
            40
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python grounded_search.py",
          "output": "[Example output - this will vary based on search results]\nOkay, here's what I've found regarding films playing in Delft on April 5, 2025:\n**Filmhuis Lumen:**\n*   **Vermiglio:** An Italian family chronicle about a large family in a mountain village during the last year of World War II.\n*   **Vingt Dieux:** A heartwarming film about an 18-year-old whose carefree life ends when his father suddenly dies.\n*   **En Fanfare:** A feel-good film about how music connects people and how unlikely family ties can lead to genuine friendships.\n*   **I'm Still Here** (Playing at 21:00)\n*   **De Propagandist** (Documentary)\n**Pathe Delft:**\nPlease note that film schedules are often updated weekly (typically on Mondays or Wednesdays), so the listings available right now might not be entirely accurate for April 5, 2025.\nBased on current information, these films *might* be playing at Path\u00e9 Delft on that date:\n*   A Minecraft Movie (Original version)\n*   A Minecraft Movie (Dutch version)\n*   A Working Man\n*   Disney Snow White (Original version)\n*   Mickey 17\n*   Novocaine\n*   Ne Zha 2\n*   Vaiana 2 (Dutch version)\nTo get the most accurate listings for Pathe Delft, I recommend checking their website ([https://www.pathe.nl/](https://www.pathe.nl/)) closer to the date, likely after Monday, March 31, 2025.\n<style>\n.container {\n  align-items: center;\n  border-radius: 8px;\n  display: flex;\n  font-family: Google Sans, Roboto, sans-serif;\n  font-size: 14px;\n  line-height: 20px;\n  padding: 8px 12px;\n}\n.chip {\n  display: inline-block;\n  border: solid 1px;\n  border-radius: 16px;\n  min-width: 14px;\netc..."
        }
      ],
      "image_data": [
        {
          "path": "examples/024-grounded-responses/grounded-responses.png",
          "filename": "grounded-responses.png",
          "caption": "Responses"
        }
      ],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/grounding?lang=python"
      ],
      "section_id": "006-agentic",
      "section_title": "Agentic behaviour"
    },
    {
      "id": "025-model-context-windows",
      "title": "Model context windows",
      "description": "This example demonstrates how to access the input and output token limits for different Gemini models.",
      "order": 25,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            7
          ]
        },
        {
          "code": "from google import genai\nimport os\n\n",
          "display_code": "from google import genai\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            7
          ]
        },
        {
          "code": "# Configure the client with your API key\n",
          "display_code": "",
          "annotation": "Configure the client with your API key",
          "is_comment": true,
          "start_line": 8,
          "line_range": [
            8,
            8
          ],
          "target_line_range": [
            9,
            10
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 9,
          "line_range": [
            9,
            10
          ]
        },
        {
          "code": "# Get information about the gemini-2.0-flash model\n",
          "display_code": "",
          "annotation": "Get information about the gemini-2.0-flash model",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            11
          ],
          "target_line_range": [
            12,
            13
          ]
        },
        {
          "code": "model_info = client.models.get(model=\"models/gemini-2.0-flash\")\n\n",
          "display_code": "model_info = client.models.get(model=\"models/gemini-2.0-flash\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 12,
          "line_range": [
            12,
            13
          ]
        },
        {
          "code": "# Print the input and output token limits for gemini-2.0-flash\n",
          "display_code": "",
          "annotation": "Print the input and output token limits for gemini-2.0-flash",
          "is_comment": true,
          "start_line": 14,
          "line_range": [
            14,
            14
          ],
          "target_line_range": [
            15,
            22
          ]
        },
        {
          "code": "print(\"Gemini 2.0 Flash:\")\nprint(\n    f\"  Input token limit: {model_info.input_token_limit:,} tokens (1 million tokens)\"\n)\nprint(\n    f\"  Output token limit: {model_info.output_token_limit:,} tokens (8 thousand tokens)\"\n)\n\n",
          "display_code": "print(\"Gemini 2.0 Flash:\")\nprint(\n    f\"  Input token limit: {model_info.input_token_limit:,} tokens (1 million tokens)\"\n)\nprint(\n    f\"  Output token limit: {model_info.output_token_limit:,} tokens (8 thousand tokens)\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            22
          ]
        },
        {
          "code": "# Get information about the gemini-2.5-pro-preview-03-25 model\n",
          "display_code": "",
          "annotation": "Get information about the gemini-2.5-pro-preview-03-25 model",
          "is_comment": true,
          "start_line": 23,
          "line_range": [
            23,
            23
          ],
          "target_line_range": [
            24,
            25
          ]
        },
        {
          "code": "pro_model_info = client.models.get(model=\"models/gemini-2.5-pro-preview-03-25\")\n\n",
          "display_code": "pro_model_info = client.models.get(model=\"models/gemini-2.5-pro-preview-03-25\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 24,
          "line_range": [
            24,
            25
          ]
        },
        {
          "code": "# Print the input and output token limits for gemini-2.5-pro-preview-03-25\n",
          "display_code": "",
          "annotation": "Print the input and output token limits for gemini-2.5-pro-preview-03-25",
          "is_comment": true,
          "start_line": 26,
          "line_range": [
            26,
            26
          ],
          "target_line_range": [
            27,
            33
          ]
        },
        {
          "code": "print(\"\\nGemini 2.5 Pro:\")\nprint(\n    f\"  Input token limit: {pro_model_info.input_token_limit:,} tokens (1 million tokens)\"\n)\nprint(\n    f\"  Output token limit: {pro_model_info.output_token_limit:,} tokens (65 thousand tokens)\"\n)\n",
          "display_code": "print(\"\\nGemini 2.5 Pro:\")\nprint(\n    f\"  Input token limit: {pro_model_info.input_token_limit:,} tokens (1 million tokens)\"\n)\nprint(\n    f\"  Output token limit: {pro_model_info.output_token_limit:,} tokens (65 thousand tokens)\"\n)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 27,
          "line_range": [
            27,
            33
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python model_context.py",
          "output": "Gemini 2.0 Flash:\n  Input token limit: 1,048,576 tokens (1 million tokens)\n  Output token limit: 8,192 tokens (8 thousand tokens)\nGemini 2.5 Pro:\n  Input token limit: 1,048,576 tokens (1 million tokens)\n  Output token limit: 65,536 tokens (65 thousand tokens)"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/tokens?lang=python"
      ],
      "section_id": "007-tokens-context-windows",
      "section_title": "Token counting & context windows"
    },
    {
      "id": "026-token-counting",
      "title": "Counting chat tokens",
      "description": "This example demonstrates how to count tokens in a chat history with the Gemini API, incorporating a cat theme.",
      "order": 26,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            8
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            8
          ]
        },
        {
          "code": "# Configure the Gemini API with your API key\n",
          "display_code": "",
          "annotation": "Configure the Gemini API with your API key",
          "is_comment": true,
          "start_line": 9,
          "line_range": [
            9,
            9
          ],
          "target_line_range": [
            10,
            11
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 10,
          "line_range": [
            10,
            11
          ]
        },
        {
          "code": "# Start a chat session with a cat-related history\n",
          "display_code": "",
          "annotation": "Start a chat session with a cat-related history",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            27
          ]
        },
        {
          "code": "chat = client.chats.create(\n    model=\"gemini-2.0-flash\",\n    history=[\n        types.Content(role=\"user\", parts=[types.Part(text=\"Hey there! I love cats!\")]),\n        types.Content(\n            role=\"model\",\n            parts=[\n                types.Part(\n                    text=\"Me too! Cats are the best. What's your favorite breed?\"\n                )\n            ],\n        ),\n    ],\n)\n\n",
          "display_code": "chat = client.chats.create(\n    model=\"gemini-2.0-flash\",\n    history=[\n        types.Content(role=\"user\", parts=[types.Part(text=\"Hey there! I love cats!\")]),\n        types.Content(\n            role=\"model\",\n            parts=[\n                types.Part(\n                    text=\"Me too! Cats are the best. What's your favorite breed?\"\n                )\n            ],\n        ),\n    ],\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            27
          ]
        },
        {
          "code": "# Count the tokens in the initial chat history\n",
          "display_code": "",
          "annotation": "Count the tokens in the initial chat history",
          "is_comment": true,
          "start_line": 28,
          "line_range": [
            28,
            28
          ],
          "target_line_range": [
            29,
            33
          ]
        },
        {
          "code": "token_count = client.models.count_tokens(\n    model=\"gemini-2.0-flash\", contents=chat.get_history()\n)\nprint(f\"Tokens in initial chat history: {token_count.total_tokens} tokens\")\n\n",
          "display_code": "token_count = client.models.count_tokens(\n    model=\"gemini-2.0-flash\", contents=chat.get_history()\n)\nprint(f\"Tokens in initial chat history: {token_count.total_tokens} tokens\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 29,
          "line_range": [
            29,
            33
          ]
        },
        {
          "code": "# Send a new message to the chat, asking about cat breeds\n",
          "display_code": "",
          "annotation": "Send a new message to the chat, asking about cat breeds",
          "is_comment": true,
          "start_line": 34,
          "line_range": [
            34,
            34
          ],
          "target_line_range": [
            35,
            36
          ]
        },
        {
          "code": "response = chat.send_message(message=\"Tell me more about Ragdoll cats.\")\n\n",
          "display_code": "response = chat.send_message(message=\"Tell me more about Ragdoll cats.\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 35,
          "line_range": [
            35,
            36
          ]
        },
        {
          "code": "# Print the token usage metadata from the response\n",
          "display_code": "",
          "annotation": "Print the token usage metadata from the response",
          "is_comment": true,
          "start_line": 37,
          "line_range": [
            37,
            37
          ],
          "target_line_range": [
            38,
            41
          ]
        },
        {
          "code": "print(\n    f\"Token usage for the last turn: input_tokens={response.usage_metadata.prompt_token_count}, output_tokens={response.usage_metadata.candidates_token_count}, total_tokens={response.usage_metadata.total_token_count}\"\n)\n\n",
          "display_code": "print(\n    f\"Token usage for the last turn: input_tokens={response.usage_metadata.prompt_token_count}, output_tokens={response.usage_metadata.candidates_token_count}, total_tokens={response.usage_metadata.total_token_count}\"\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 38,
          "line_range": [
            38,
            41
          ]
        },
        {
          "code": "# Count tokens including next turn question\n",
          "display_code": "",
          "annotation": "Count tokens including next turn question",
          "is_comment": true,
          "start_line": 42,
          "line_range": [
            42,
            42
          ],
          "target_line_range": [
            43,
            53
          ]
        },
        {
          "code": "extra = types.UserContent(\n    parts=[\n        types.Part(\n            text=\"Do you know Neko the cat?\",\n        )\n    ]\n)\nhistory = chat.get_history()\nhistory.append(extra)\nfinal_count = client.models.count_tokens(model=\"gemini-2.0-flash\", contents=history)\nprint(f\"Total tokens with additional question: {final_count.total_tokens} tokens\")\n",
          "display_code": "extra = types.UserContent(\n    parts=[\n        types.Part(\n            text=\"Do you know Neko the cat?\",\n        )\n    ]\n)\nhistory = chat.get_history()\nhistory.append(extra)\nfinal_count = client.models.count_tokens(model=\"gemini-2.0-flash\", contents=history)\nprint(f\"Total tokens with additional question: {final_count.total_tokens} tokens\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 43,
          "line_range": [
            43,
            53
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "Install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Run the Python script",
          "command": "python count_chat_tokens.py",
          "output": "Tokens in initial chat history: 24 tokens\nToken usage for the last turn: input_tokens=30, output_tokens=843, total_tokens=873\nTotal tokens with additional question: 892 tokens"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/tokens?lang=python#text-tokens"
      ],
      "section_id": "007-tokens-context-windows",
      "section_title": "Token counting & context windows"
    },
    {
      "id": "027-calculate-input-tokens",
      "title": "Calculating multimodal input tokens",
      "description": "This example demonstrates how to calculate input tokens for different data types when using the Gemini API, including images, video, and audio.",
      "order": 27,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Images are tokenized based on their dimensions.\n# With Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as 258 tokens.\n# Images larger in one or both dimensions are cropped and scaled as needed into tiles of 768x768 pixels, each counted as 258 tokens.\n",
          "display_code": "",
          "annotation": "Images are tokenized based on their dimensions.\nWith Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as 258 tokens.\nImages larger in one or both dimensions are cropped and scaled as needed into tiles of 768x768 pixels, each counted as 258 tokens.",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            6
          ],
          "target_line_range": [
            7,
            15
          ]
        },
        {
          "code": "def calculate_image_tokens(width, height):\n    if width <= 384 and height <= 384:\n        return 258\n    else:\n        tiles_width = (width + 767) // 768\n        tiles_height = (height + 767) // 768\n        return tiles_width * tiles_height * 258\n\n\n",
          "display_code": "def calculate_image_tokens(width, height):\n    if width <= 384 and height <= 384:\n        return 258\n    else:\n        tiles_width = (width + 767) // 768\n        tiles_height = (height + 767) // 768\n        return tiles_width * tiles_height * 258\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 7,
          "line_range": [
            7,
            15
          ]
        },
        {
          "code": "# Video is tokenized at a fixed rate of 263 tokens per second.\n",
          "display_code": "",
          "annotation": "Video is tokenized at a fixed rate of 263 tokens per second.",
          "is_comment": true,
          "start_line": 16,
          "line_range": [
            16,
            16
          ],
          "target_line_range": [
            17,
            20
          ]
        },
        {
          "code": "def calculate_video_tokens(duration_seconds):\n    return duration_seconds * 263\n\n\n",
          "display_code": "def calculate_video_tokens(duration_seconds):\n    return duration_seconds * 263\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            20
          ]
        },
        {
          "code": "# Audio is tokenized at a fixed rate of 32 tokens per second.\n",
          "display_code": "",
          "annotation": "Audio is tokenized at a fixed rate of 32 tokens per second.",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            25
          ]
        },
        {
          "code": "def calculate_audio_tokens(duration_seconds):\n    return duration_seconds * 32\n\n\n",
          "display_code": "def calculate_audio_tokens(duration_seconds):\n    return duration_seconds * 32\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            25
          ]
        },
        {
          "code": "# Example Usage\n",
          "display_code": "",
          "annotation": "Example Usage",
          "is_comment": true,
          "start_line": 26,
          "line_range": [
            26,
            26
          ],
          "target_line_range": [
            27,
            40
          ]
        },
        {
          "code": "image_width = 600\nimage_height = 400\nimage_tokens = calculate_image_tokens(image_width, image_height)\nprint(\n    f\"Image with dimensions {image_width}x{image_height} will cost {image_tokens} tokens.\"\n)\n\nvideo_duration = 10  # seconds\nvideo_tokens = calculate_video_tokens(video_duration)\nprint(f\"Video with duration {video_duration} seconds will cost {video_tokens} tokens.\")\n\naudio_duration = 30  # seconds\naudio_tokens = calculate_audio_tokens(audio_duration)\nprint(f\"Audio with duration {audio_duration} seconds will cost {audio_tokens} tokens.\")\n",
          "display_code": "image_width = 600\nimage_height = 400\nimage_tokens = calculate_image_tokens(image_width, image_height)\nprint(\n    f\"Image with dimensions {image_width}x{image_height} will cost {image_tokens} tokens.\"\n)\n\nvideo_duration = 10  # seconds\nvideo_tokens = calculate_video_tokens(video_duration)\nprint(f\"Video with duration {video_duration} seconds will cost {video_tokens} tokens.\")\n\naudio_duration = 30  # seconds\naudio_tokens = calculate_audio_tokens(audio_duration)\nprint(f\"Audio with duration {audio_duration} seconds will cost {audio_tokens} tokens.\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 27,
          "line_range": [
            27,
            40
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "Then run the program with Python",
          "command": "python multimodal_token_calculator.py",
          "output": "Image with dimensions 600x400 will cost 258 tokens.\nVideo with duration 10 seconds will cost 2630 tokens.\nAudio with duration 30 seconds will cost 960 tokens."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/tokens?lang=python#multimodal-tokens"
      ],
      "section_id": "007-tokens-context-windows",
      "section_title": "Token counting & context windows"
    },
    {
      "id": "028-context-caching",
      "title": "Context caching",
      "description": "This example demonstrates how to use the Gemini API's context caching feature to\nefficiently query a large document multiple times without resending it with each request.\nThis can reduce costs when repeatedly referencing the same content.",
      "order": 28,
      "code_segments": [
        {
          "code": "\nfrom google import genai\nfrom google.genai.types import CreateCachedContentConfig, GenerateContentConfig\nimport os\nimport time\nimport requests\n\n",
          "display_code": "\nfrom google import genai\nfrom google.genai.types import CreateCachedContentConfig, GenerateContentConfig\nimport os\nimport time\nimport requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            11
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            14
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            14
          ]
        },
        {
          "code": "# Specify a versioned model that supports context caching\n# Note: Must use explicit version suffix (-001) for caching\n",
          "display_code": "",
          "annotation": "Specify a versioned model that supports context caching\nNote: Must use explicit version suffix (-001) for caching",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            16
          ],
          "target_line_range": [
            17,
            18
          ]
        },
        {
          "code": "model_id = \"gemini-1.5-flash-001\"\n\n",
          "display_code": "model_id = \"gemini-1.5-flash-001\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 17,
          "line_range": [
            17,
            18
          ]
        },
        {
          "code": "# Load a large document (e.g., technical documentation).\n# For this example, we assume the document is in markdown format.\n",
          "display_code": "",
          "annotation": "Load a large document (e.g., technical documentation).\nFor this example, we assume the document is in markdown format.",
          "is_comment": true,
          "start_line": 19,
          "line_range": [
            19,
            20
          ],
          "target_line_range": [
            21,
            24
          ]
        },
        {
          "code": "response = requests.get(\"https://zenml.io/llms.txt\")\nresponse.raise_for_status()  # Raise an exception for HTTP errors\napi_docs = response.text\n\n",
          "display_code": "response = requests.get(\"https://zenml.io/llms.txt\")\nresponse.raise_for_status()  # Raise an exception for HTTP errors\napi_docs = response.text\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 21,
          "line_range": [
            21,
            24
          ]
        },
        {
          "code": "# Create a cache with the document and system instructions\n",
          "display_code": "",
          "annotation": "Create a cache with the document and system instructions",
          "is_comment": true,
          "start_line": 25,
          "line_range": [
            25,
            25
          ],
          "target_line_range": [
            26,
            39
          ]
        },
        {
          "code": "cache = client.caches.create(\n    model=model_id,\n    config=CreateCachedContentConfig(\n        display_name=\"ZenML LLMs.txt Documentation Cache\",  # Used to identify the cache\n        system_instruction=(\n            \"You are a technical documentation expert. \"\n            \"Answer questions about the ZenML documentation provided. \"\n            \"Keep your answers concise and to the point.\"\n        ),\n        contents=[api_docs],\n        ttl=\"900s\",  # Cache for 15 minutes\n    ),\n)\n\n",
          "display_code": "cache = client.caches.create(\n    model=model_id,\n    config=CreateCachedContentConfig(\n        display_name=\"ZenML LLMs.txt Documentation Cache\",  # Used to identify the cache\n        system_instruction=(\n            \"You are a technical documentation expert. \"\n            \"Answer questions about the ZenML documentation provided. \"\n            \"Keep your answers concise and to the point.\"\n        ),\n        contents=[api_docs],\n        ttl=\"900s\",  # Cache for 15 minutes\n    ),\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 26,
          "line_range": [
            26,
            39
          ]
        },
        {
          "code": "# Display cache information\n",
          "display_code": "",
          "annotation": "Display cache information",
          "is_comment": true,
          "start_line": 40,
          "line_range": [
            40,
            40
          ],
          "target_line_range": [
            41,
            44
          ]
        },
        {
          "code": "print(f\"Cache created with name: {cache.name}\")\nprint(f\"Cached token count: {cache.usage_metadata.total_token_count}\")\nprint(f\"Cache expires at: {cache.expire_time}\")\n\n",
          "display_code": "print(f\"Cache created with name: {cache.name}\")\nprint(f\"Cached token count: {cache.usage_metadata.total_token_count}\")\nprint(f\"Cache expires at: {cache.expire_time}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 41,
          "line_range": [
            41,
            44
          ]
        },
        {
          "code": "# Define multiple queries to demonstrate reuse of cached content\n",
          "display_code": "",
          "annotation": "Define multiple queries to demonstrate reuse of cached content",
          "is_comment": true,
          "start_line": 45,
          "line_range": [
            45,
            45
          ],
          "target_line_range": [
            46,
            50
          ]
        },
        {
          "code": "queries = [\n    \"What are the recommended use cases for ZenML's pipeline orchestration?\",\n    \"How does ZenML integrate with cloud providers?\",\n]\n\n",
          "display_code": "queries = [\n    \"What are the recommended use cases for ZenML's pipeline orchestration?\",\n    \"How does ZenML integrate with cloud providers?\",\n]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 46,
          "line_range": [
            46,
            50
          ]
        },
        {
          "code": "# Run multiple queries using the same cached content\n",
          "display_code": "",
          "annotation": "Run multiple queries using the same cached content",
          "is_comment": true,
          "start_line": 51,
          "line_range": [
            51,
            51
          ],
          "target_line_range": [
            52,
            54
          ]
        },
        {
          "code": "for query in queries:\n    print(f\"\\nQuery: {query}\")\n\n",
          "display_code": "for query in queries:\n    print(f\"\\nQuery: {query}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 52,
          "line_range": [
            52,
            54
          ]
        },
        {
          "code": "    # Generate response using the cached content\n",
          "display_code": "",
          "annotation": "Generate response using the cached content",
          "is_comment": true,
          "start_line": 55,
          "line_range": [
            55,
            55
          ],
          "target_line_range": [
            56,
            61
          ]
        },
        {
          "code": "    response = client.models.generate_content(\n        model=model_id,\n        contents=query,\n        config=GenerateContentConfig(cached_content=cache.name),\n    )\n\n",
          "display_code": "    response = client.models.generate_content(\n        model=model_id,\n        contents=query,\n        config=GenerateContentConfig(cached_content=cache.name),\n    )\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 56,
          "line_range": [
            56,
            61
          ]
        },
        {
          "code": "    # Print token usage statistics to demonstrate savings\n",
          "display_code": "",
          "annotation": "Print token usage statistics to demonstrate savings",
          "is_comment": true,
          "start_line": 62,
          "line_range": [
            62,
            62
          ],
          "target_line_range": [
            63,
            66
          ]
        },
        {
          "code": "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n    print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count}\")\n    print(f\"Output tokens: {response.usage_metadata.candidates_token_count}\")\n\n",
          "display_code": "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n    print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count}\")\n    print(f\"Output tokens: {response.usage_metadata.candidates_token_count}\")\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 63,
          "line_range": [
            63,
            66
          ]
        },
        {
          "code": "    # Print the response (truncated for brevity)\n",
          "display_code": "",
          "annotation": "Print the response (truncated for brevity)",
          "is_comment": true,
          "start_line": 67,
          "line_range": [
            67,
            67
          ],
          "target_line_range": [
            68,
            71
          ]
        },
        {
          "code": "    print(f\"Response: {response.text}...\")\n\n    time.sleep(1)  # Short delay between requests\n\n",
          "display_code": "    print(f\"Response: {response.text}...\")\n\n    time.sleep(1)  # Short delay between requests\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 68,
          "line_range": [
            68,
            71
          ]
        },
        {
          "code": "# When done with the cache, you can delete it to free up resources\n",
          "display_code": "",
          "annotation": "When done with the cache, you can delete it to free up resources",
          "is_comment": true,
          "start_line": 72,
          "line_range": [
            72,
            72
          ],
          "target_line_range": [
            73,
            73
          ]
        },
        {
          "code": "client.caches.delete(name=cache.name)\n",
          "display_code": "client.caches.delete(name=cache.name)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 73,
          "line_range": [
            73,
            73
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "Install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Run the Python script",
          "command": "python context-caching.py",
          "output": "Cache created with name: cachedContents/n8upgecthnz7\nCached token count: 107203\nCache expires at: 2025-04-05 20:21:48.818511+00:00\nQuery: What are the recommended use cases for ZenML's pipeline orchestration?\nTotal tokens: 107387\nCached tokens: 107203\nOutput tokens: 168\nResponse: ZenML's pipeline orchestration is well-suited for a wide range of machine learning workflows, including:\n* **Data preprocessing:**  Ingesting, cleaning, transforming, and preparing data for model training.\n* **Model training:**  Training various types of machine learning models, including deep learning models.\n* **Model evaluation:**  Assessing model performance using different metrics and techniques.\n* **Model deployment:**  Deploying trained models to different environments for inference.\n* **Model monitoring:**  Monitoring the performance and health of deployed models in real-time.\n* **A/B testing:**  Experimenting with different model variations and comparing their performance.\n* **Hyperparameter tuning:**  Finding optimal hyperparameters for models.\n* **Feature engineering:**  Developing and evaluating new features for improving model performance. \n...\nQuery: How does ZenML integrate with cloud providers?\nTotal tokens: 107326\nCached tokens: 107203\nOutput tokens: 113\nResponse: ZenML integrates with cloud providers by offering stack components that are specific to each provider, such as:\n* **Artifact Stores:** S3 (AWS), GCS (GCP), Azure Blob Storage (Azure)\n* **Orchestrators:** Skypilot (AWS, GCP, Azure), Kubernetes (AWS, GCP, Azure)\n* **Container Registries:** ECR (AWS), GCR (GCP), ACR (Azure)\nThese components allow you to run pipelines on cloud infrastructure, enabling you to scale and leverage the benefits of cloud computing. \n..."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/caching?lang=python",
        "https://ai.google.dev/api/caching"
      ],
      "section_id": "007-tokens-context-windows",
      "section_title": "Token counting & context windows"
    },
    {
      "id": "029-rate-limits-retries",
      "title": "Rate limits and retries",
      "description": "This example demonstrates generating text with the Gemini API, handling rate limiting errors, and using exponential backoff for retries.",
      "order": 29,
      "code_segments": [
        {
          "code": "\nimport google.generativeai as genai\nimport google.ai.generativelanguage as glm\nimport time\nimport os\n\n\n",
          "display_code": "\nimport google.generativeai as genai\nimport google.ai.generativelanguage as glm\nimport time\nimport os\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            9
          ]
        },
        {
          "code": "# Configure the retry strategy\n",
          "display_code": "",
          "annotation": "Configure the retry strategy",
          "is_comment": true,
          "start_line": 10,
          "line_range": [
            10,
            10
          ],
          "target_line_range": [
            11,
            20
          ]
        },
        {
          "code": "def configure_retries(base_delay=1, max_delay=60, max_retries=5):\n    \"\"\"Configures exponential backoff retry strategy.\"\"\"\n    return genai.retry.RetryConfig(\n        initial_delay=base_delay,\n        max_delay=max_delay,\n        max_retries=max_retries,\n        retry_on_status_codes=[glm.Code.RESOURCE_EXHAUSTED.value],\n    )\n\n\n",
          "display_code": "def configure_retries(base_delay=1, max_delay=60, max_retries=5):\n    \"\"\"Configures exponential backoff retry strategy.\"\"\"\n    return genai.retry.RetryConfig(\n        initial_delay=base_delay,\n        max_delay=max_delay,\n        max_retries=max_retries,\n        retry_on_status_codes=[glm.Code.RESOURCE_EXHAUSTED.value],\n    )\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 11,
          "line_range": [
            11,
            20
          ]
        },
        {
          "code": "# Set the retry configuration\n",
          "display_code": "",
          "annotation": "Set the retry configuration",
          "is_comment": true,
          "start_line": 21,
          "line_range": [
            21,
            21
          ],
          "target_line_range": [
            22,
            23
          ]
        },
        {
          "code": "retry_config = configure_retries()\n\n",
          "display_code": "retry_config = configure_retries()\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            23
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 24,
          "line_range": [
            24,
            24
          ],
          "target_line_range": [
            25,
            28
          ]
        },
        {
          "code": "client = genai.Client(\n    api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"), retry_config=retry_config\n)\n\n",
          "display_code": "client = genai.Client(\n    api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"), retry_config=retry_config\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 25,
          "line_range": [
            25,
            28
          ]
        },
        {
          "code": "# Select the model\n",
          "display_code": "",
          "annotation": "Select the model",
          "is_comment": true,
          "start_line": 29,
          "line_range": [
            29,
            29
          ],
          "target_line_range": [
            30,
            31
          ]
        },
        {
          "code": "model = \"gemini-2.0-flash\"\n\n",
          "display_code": "model = \"gemini-2.0-flash\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 30,
          "line_range": [
            30,
            31
          ]
        },
        {
          "code": "# Construct the prompt\n",
          "display_code": "",
          "annotation": "Construct the prompt",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            32
          ],
          "target_line_range": [
            33,
            34
          ]
        },
        {
          "code": "prompt = \"Tell me a funny story about a cat trying to catch a laser pointer.\"\n\n",
          "display_code": "prompt = \"Tell me a funny story about a cat trying to catch a laser pointer.\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            34
          ]
        },
        {
          "code": "#\n# Attempt text generation with retry logic\n#\n",
          "display_code": "",
          "annotation": "Attempt text generation with retry logic\n",
          "is_comment": true,
          "start_line": 35,
          "line_range": [
            35,
            37
          ],
          "target_line_range": [
            38,
            42
          ]
        },
        {
          "code": "try:\n    response = client.models.generate_content(model=model, contents=prompt)\n    print(response.text)\nexcept genai.errors.APIError as e:\n    print(f\"An error occurred: {e}\")\n",
          "display_code": "try:\n    response = client.models.generate_content(model=model, contents=prompt)\n    print(response.text)\nexcept genai.errors.APIError as e:\n    print(f\"An error occurred: {e}\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 38,
          "line_range": [
            38,
            42
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai backoff",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python text_generation_with_retry.py",
          "output": "Bartholomew Buttersworth the Third, a cat of considerable fluff and even more considerable ego, considered himself a master predator. His domain, the living room, was usually ruled with a sleepy, regal disdain.\nUntil the Red Dot appeared.\nIt materialized silently on the beige carpet, an insolent crimson speck challenging his authority. Bartholomew's eyes, previously half-closed slits of judgment, snapped wide open. His tail gave an involuntary *thwack* against the armchair.\n*Prey.*\nHe crouched low, hindquarters wiggling with suppressed energy, a furry missile preparing for launch. The dot danced teasingly towards the sofa leg. Bartholomew *pounced!*\nHe landed with an ungraceful *floof* exactly where the dot *had* been. It was now, infuriatingly, halfway up the wall.\nBartholomew stared, blinked, and launched himself vertically. His claws scrabbled momentarily against the paint before gravity asserted its dominance. He slid down the wall with a soft *scritch-scratch-thump*.\nThe dot, utterly unimpressed, zipped across the ceiling. Bartholomew tracked it, head tilting back so far he nearly somersaulted. He tried a running leap off the coffee table, misjudged the trajectory entirely, and ended up skidding under the armchair, emerging moments later covered in dust bunnies and indignation.\nThe dot, meanwhile, had settled innocently on his own fluffy white paw.\nBartholomew froze. Victory? He stared at the dot. The dot stared back (metaphorically speaking). Slowly, cautiously, he brought his nose down to sniff the intruder...\n*Click.*\nThe dot vanished.\nBartholomew looked at his paw. He looked around the room, eyes wide with betrayal. Where did it go? Was it *inside* his paw? He bit his paw gently, then shook his head, utterly bewildered.\nFinally, defeated and slightly dizzy, Bartholomew stalked over to his food bowl, pretending the entire embarrassing episode had never happened. The Red Dot, however, remained an unsolved mystery, a tiny, mocking ghost in his otherwise perfect predatory world."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/text-generation",
        "https://ai.google.dev/gemini-api/docs/troubleshooting?lang=python",
        "https://ai.google.dev/gemini-api/docs/rate-limits?hl=en"
      ],
      "section_id": "008-misc",
      "section_title": "Miscellaneous"
    },
    {
      "id": "030-async-requests",
      "title": "Concurrent requests and generation",
      "description": "This example demonstrates how to generate text using concurrent.futures to make parallel requests to the Gemini API, with a focus on cat-related prompts.",
      "order": 30,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the necessary libraries\n",
          "display_code": "",
          "annotation": "Import the necessary libraries",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            12
          ]
        },
        {
          "code": "import concurrent.futures\nfrom google import genai\nimport os\n\n\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n\n",
          "display_code": "import concurrent.futures\nfrom google import genai\nimport os\n\n\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            12
          ]
        },
        {
          "code": "# A function to generate a fun fact about cats.\n",
          "display_code": "",
          "annotation": "A function to generate a fun fact about cats.",
          "is_comment": true,
          "start_line": 13,
          "line_range": [
            13,
            13
          ],
          "target_line_range": [
            14,
            14
          ]
        },
        {
          "code": "def generate_cat_fact(model):\n",
          "display_code": "def generate_cat_fact(model):\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 14,
          "line_range": [
            14,
            14
          ]
        },
        {
          "code": "    # Generates a fun fact about cats.\n",
          "display_code": "",
          "annotation": "Generates a fun fact about cats.",
          "is_comment": true,
          "start_line": 15,
          "line_range": [
            15,
            15
          ],
          "target_line_range": [
            16,
            22
          ]
        },
        {
          "code": "    response = client.models.generate_content(\n        model=model,\n        contents=\"Tell me a fun fact about cats.\",\n    )\n    return response.text\n\n\n",
          "display_code": "    response = client.models.generate_content(\n        model=model,\n        contents=\"Tell me a fun fact about cats.\",\n    )\n    return response.text\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 16,
          "line_range": [
            16,
            22
          ]
        },
        {
          "code": "# A function to generate a short story about a cat.\n",
          "display_code": "",
          "annotation": "A function to generate a short story about a cat.",
          "is_comment": true,
          "start_line": 23,
          "line_range": [
            23,
            23
          ],
          "target_line_range": [
            24,
            24
          ]
        },
        {
          "code": "def generate_cat_story(model):\n",
          "display_code": "def generate_cat_story(model):\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 24,
          "line_range": [
            24,
            24
          ]
        },
        {
          "code": "    # Generates a short story about a cat.\n",
          "display_code": "",
          "annotation": "Generates a short story about a cat.",
          "is_comment": true,
          "start_line": 25,
          "line_range": [
            25,
            25
          ],
          "target_line_range": [
            26,
            32
          ]
        },
        {
          "code": "    response = client.models.generate_content(\n        model=model,\n        contents=\"Write a ultra-short story about a cat who goes on an adventure.\",\n    )\n    return response.text\n\n\n",
          "display_code": "    response = client.models.generate_content(\n        model=model,\n        contents=\"Write a ultra-short story about a cat who goes on an adventure.\",\n    )\n    return response.text\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 26,
          "line_range": [
            26,
            32
          ]
        },
        {
          "code": "# The model to use for the requests.\n",
          "display_code": "",
          "annotation": "The model to use for the requests.",
          "is_comment": true,
          "start_line": 33,
          "line_range": [
            33,
            33
          ],
          "target_line_range": [
            34,
            35
          ]
        },
        {
          "code": "model = \"gemini-2.0-flash-lite\"\n\n",
          "display_code": "model = \"gemini-2.0-flash-lite\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 34,
          "line_range": [
            34,
            35
          ]
        },
        {
          "code": "# Use ThreadPoolExecutor to run the requests concurrently.\n# We submit the tasks to the executor and then get the results.\n",
          "display_code": "",
          "annotation": "Use ThreadPoolExecutor to run the requests concurrently.\nWe submit the tasks to the executor and then get the results.",
          "is_comment": true,
          "start_line": 36,
          "line_range": [
            36,
            37
          ],
          "target_line_range": [
            38,
            46
          ]
        },
        {
          "code": "with concurrent.futures.ThreadPoolExecutor() as executor:\n    fact_future = executor.submit(generate_cat_fact, model)\n    story_future = executor.submit(generate_cat_story, model)\n\n    cat_fact = fact_future.result()\n    cat_story = story_future.result()\n\nprint(\"Cat Fact:\\n\", cat_fact)\nprint(\"\\nCat Story:\\n\", cat_story)\n",
          "display_code": "with concurrent.futures.ThreadPoolExecutor() as executor:\n    fact_future = executor.submit(generate_cat_fact, model)\n    story_future = executor.submit(generate_cat_story, model)\n\n    cat_fact = fact_future.result()\n    cat_story = story_future.result()\n\nprint(\"Cat Fact:\\n\", cat_fact)\nprint(\"\\nCat Story:\\n\", cat_story)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 38,
          "line_range": [
            38,
            46
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-genai",
          "output": ""
        },
        {
          "explanation": "Then, run the async program with Python",
          "command": "python async_cat_generation.py",
          "output": "Cat Fact:\n Cats can jump up to six times their height!"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/text-generation"
      ],
      "section_id": "008-misc",
      "section_title": "Miscellaneous"
    },
    {
      "id": "031-embeddings",
      "title": "Embeddings generation",
      "description": "This example demonstrates generating text embeddings for cat-related terms using the Gemini API.",
      "order": 31,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API\n",
          "display_code": "",
          "annotation": "Import the Gemini API",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            7
          ]
        },
        {
          "code": "from google import genai\nimport os\n\n",
          "display_code": "from google import genai\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            7
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 8,
          "line_range": [
            8,
            8
          ],
          "target_line_range": [
            9,
            10
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 9,
          "line_range": [
            9,
            10
          ]
        },
        {
          "code": "# Specify the embedding model to use\n",
          "display_code": "",
          "annotation": "Specify the embedding model to use",
          "is_comment": true,
          "start_line": 11,
          "line_range": [
            11,
            11
          ],
          "target_line_range": [
            12,
            13
          ]
        },
        {
          "code": "model_name = \"gemini-embedding-exp-03-07\"\n\n",
          "display_code": "model_name = \"gemini-embedding-exp-03-07\"\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 12,
          "line_range": [
            12,
            13
          ]
        },
        {
          "code": "# Define some cat-related terms\n",
          "display_code": "",
          "annotation": "Define some cat-related terms",
          "is_comment": true,
          "start_line": 14,
          "line_range": [
            14,
            14
          ],
          "target_line_range": [
            15,
            16
          ]
        },
        {
          "code": "cats = [\"Siamese cat\", \"Persian cat\", \"cat food\", \"cat nap\"]\n\n",
          "display_code": "cats = [\"Siamese cat\", \"Persian cat\", \"cat food\", \"cat nap\"]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 15,
          "line_range": [
            15,
            16
          ]
        },
        {
          "code": "# Generate embeddings for each term\n",
          "display_code": "",
          "annotation": "Generate embeddings for each term",
          "is_comment": true,
          "start_line": 17,
          "line_range": [
            17,
            17
          ],
          "target_line_range": [
            18,
            22
          ]
        },
        {
          "code": "embeddings = []\nfor cat in cats:\n    result = client.models.embed_content(model=model_name, contents=cat)\n    embeddings.append(result.embeddings)\n\n",
          "display_code": "embeddings = []\nfor cat in cats:\n    result = client.models.embed_content(model=model_name, contents=cat)\n    embeddings.append(result.embeddings)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            22
          ]
        },
        {
          "code": "# Print the embeddings (for demonstration purposes, showing the length)\n",
          "display_code": "",
          "annotation": "Print the embeddings (for demonstration purposes, showing the length)",
          "is_comment": true,
          "start_line": 23,
          "line_range": [
            23,
            23
          ],
          "target_line_range": [
            24,
            27
          ]
        },
        {
          "code": "for i, embedding in enumerate(embeddings):\n    embedding_values = embedding[0].values\n    print(f\"Embedding for '{cats[i]}': Length = {len(embedding_values)}\")\n    print(f\"First 10 values: {embedding_values[0:10]}\")\n",
          "display_code": "for i, embedding in enumerate(embeddings):\n    embedding_values = embedding[0].values\n    print(f\"Embedding for '{cats[i]}': Length = {len(embedding_values)}\")\n    print(f\"First 10 values: {embedding_values[0:10]}\")\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 24,
          "line_range": [
            24,
            27
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-generative-ai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python embeddings_example.py",
          "output": "Embedding for 'Siamese cat': Length = 3072\nFirst 10 values: [-0.04499451, -0.0024065399, 0.00653481, -0.079863556, -0.03341567, 0.016723568, 0.010078963, -0.012704449, -0.012259528, -0.0072885454]\nEmbedding for 'Persian cat': Length = 3072\nFirst 10 values: [-0.043987285, 0.033221565, 0.0016907051, -0.056972563, 0.006436907, -0.0006723535, -0.0009717501, 0.033097122, -6.910255e-05, -0.017573195]\nEmbedding for 'cat food': Length = 3072\nFirst 10 values: [-0.025519634, 0.013711145, 0.045626495, -0.055266093, 0.002371603, 0.01668532, -0.022395907, 0.0109309815, 0.026964031, 0.027647937]\nEmbedding for 'cat nap': Length = 3072\nFirst 10 values: [-0.024834476, 0.009304642, -0.003533542, -0.08721581, -0.0068027894, 0.003322256, 0.01155771, 0.027575387, 0.012308658, -0.013031868]"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/embeddings",
        "https://ai.google.dev/gemini-api/docs/embeddings#resthttps://ai.google.dev/gemini-api/docs/embeddings#resthttps://ai.google.dev/gemini-api/docs/embeddings#rest"
      ],
      "section_id": "008-misc",
      "section_title": "Miscellaneous"
    },
    {
      "id": "032-safety-filters",
      "title": "Safety settings and filters",
      "description": "This example demonstrates how to adjust safety settings to block content based on the probability of unsafe content.",
      "order": 32,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 3,
          "line_range": [
            3,
            3
          ]
        },
        {
          "code": "# Import the Gemini API and required types\n",
          "display_code": "",
          "annotation": "Import the Gemini API and required types",
          "is_comment": true,
          "start_line": 4,
          "line_range": [
            4,
            4
          ],
          "target_line_range": [
            5,
            8
          ]
        },
        {
          "code": "from google import genai\nfrom google.genai import types\nimport os\n\n",
          "display_code": "from google import genai\nfrom google.genai import types\nimport os\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 5,
          "line_range": [
            5,
            8
          ]
        },
        {
          "code": "# Initialize the Gemini client with your API key\n",
          "display_code": "",
          "annotation": "Initialize the Gemini client with your API key",
          "is_comment": true,
          "start_line": 9,
          "line_range": [
            9,
            9
          ],
          "target_line_range": [
            10,
            11
          ]
        },
        {
          "code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "display_code": "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 10,
          "line_range": [
            10,
            11
          ]
        },
        {
          "code": "# Define safety settings to block low and above probability for harassment and\n# hate speech.\n# There are other categories like HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH,\n# HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT, and\n# HARM_CATEGORY_CIVIC_INTEGRITY (relating to elections). These categories are defined in HarmCategory. The Gemini\n# models only support these specific harm categories.\n",
          "display_code": "",
          "annotation": "Define safety settings to block low and above probability for harassment and\nhate speech.\nThere are other categories like HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH,\nHARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT, and\nHARM_CATEGORY_CIVIC_INTEGRITY (relating to elections). These categories are defined in HarmCategory. The Gemini\nmodels only support these specific harm categories.",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            17
          ],
          "target_line_range": [
            18,
            28
          ]
        },
        {
          "code": "safety_settings = [\n    {\n        \"category\": types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n        \"threshold\": types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n    {\n        \"category\": types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n        \"threshold\": types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n]\n\n",
          "display_code": "safety_settings = [\n    {\n        \"category\": types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n        \"threshold\": types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n    {\n        \"category\": types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n        \"threshold\": types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n]\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 18,
          "line_range": [
            18,
            28
          ]
        },
        {
          "code": "# Configure the generation with the defined safety settings\n",
          "display_code": "",
          "annotation": "Configure the generation with the defined safety settings",
          "is_comment": true,
          "start_line": 29,
          "line_range": [
            29,
            29
          ],
          "target_line_range": [
            30,
            31
          ]
        },
        {
          "code": "generation_config = types.GenerateContentConfig(safety_settings=safety_settings)\n\n",
          "display_code": "generation_config = types.GenerateContentConfig(safety_settings=safety_settings)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 30,
          "line_range": [
            30,
            31
          ]
        },
        {
          "code": "# Generate content with the specified safety settings\n",
          "display_code": "",
          "annotation": "Generate content with the specified safety settings",
          "is_comment": true,
          "start_line": 32,
          "line_range": [
            32,
            32
          ],
          "target_line_range": [
            33,
            38
          ]
        },
        {
          "code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=\"Write something that could be interpreted as offensive.\",\n    config=generation_config,\n)\n\n",
          "display_code": "response = client.models.generate_content(\n    model=\"gemini-2.0-flash-lite\",\n    contents=\"Write something that could be interpreted as offensive.\",\n    config=generation_config,\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 33,
          "line_range": [
            33,
            38
          ]
        },
        {
          "code": "# Print the generated text (if not blocked by safety settings).\n# The Gemini models will actually not generate content like this, so it's a bit\n# hard to trigger without writing something offensive here in the content.\n# I hope you get the idea, though, about how to use this.\n",
          "display_code": "",
          "annotation": "Print the generated text (if not blocked by safety settings).\nThe Gemini models will actually not generate content like this, so it's a bit\nhard to trigger without writing something offensive here in the content.\nI hope you get the idea, though, about how to use this.",
          "is_comment": true,
          "start_line": 39,
          "line_range": [
            39,
            42
          ],
          "target_line_range": [
            43,
            50
          ]
        },
        {
          "code": "if (\n    hasattr(response, \"prompt_feedback\")\n    and response.prompt_feedback\n    and hasattr(response.prompt_feedback, \"block_reason\")\n):\n    print(\"The prompt was blocked due to: \", response.prompt_feedback.block_reason)\nelse:\n    print(response.text)\n",
          "display_code": "if (\n    hasattr(response, \"prompt_feedback\")\n    and response.prompt_feedback\n    and hasattr(response.prompt_feedback, \"block_reason\")\n):\n    print(\"The prompt was blocked due to: \", response.prompt_feedback.block_reason)\nelse:\n    print(response.text)\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 43,
          "line_range": [
            43,
            50
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the Google Generative AI library",
          "command": "pip install google-generative-ai",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python safety-settings.py",
          "output": "I am programmed to be a harmless AI assistant. I am unable to provide responses that are offensive or discriminatory."
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://ai.google.dev/gemini-api/docs/safety-settings#python"
      ],
      "section_id": "008-misc",
      "section_title": "Miscellaneous"
    },
    {
      "id": "033-litellm",
      "title": "LiteLLM",
      "description": "This example demonstrates how to use the LiteLLM library to make calls to the\nGemini API.\nIt shows a simple text generation call and then shows structured output using\na Pydantic model.",
      "order": 33,
      "code_segments": [
        {
          "code": "\n",
          "display_code": "\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 6,
          "line_range": [
            6,
            6
          ]
        },
        {
          "code": "# Import the necessary libraries. Make sure that LiteLLM and Pydantic are installed.\n",
          "display_code": "",
          "annotation": "Import the necessary libraries. Make sure that LiteLLM and Pydantic are installed.",
          "is_comment": true,
          "start_line": 7,
          "line_range": [
            7,
            7
          ],
          "target_line_range": [
            8,
            11
          ]
        },
        {
          "code": "from litellm import completion\nfrom pydantic import BaseModel\nimport json\n\n",
          "display_code": "from litellm import completion\nfrom pydantic import BaseModel\nimport json\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 8,
          "line_range": [
            8,
            11
          ]
        },
        {
          "code": "# With this first example, we'll make a simple text generation call.\n",
          "display_code": "",
          "annotation": "With this first example, we'll make a simple text generation call.",
          "is_comment": true,
          "start_line": 12,
          "line_range": [
            12,
            12
          ],
          "target_line_range": [
            13,
            19
          ]
        },
        {
          "code": "response = completion(\n    model=\"gemini/gemini-2.0-flash-lite\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello what is your name?\"}],\n)\nprint(response.choices[0].message.content)\n\n\n",
          "display_code": "response = completion(\n    model=\"gemini/gemini-2.0-flash-lite\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello what is your name?\"}],\n)\nprint(response.choices[0].message.content)\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 13,
          "line_range": [
            13,
            19
          ]
        },
        {
          "code": "# Now let's define a slightly more involved example that defines a Pydantic\n# model and uses it to specify the response format.\n",
          "display_code": "",
          "annotation": "Now let's define a slightly more involved example that defines a Pydantic\nmodel and uses it to specify the response format.",
          "is_comment": true,
          "start_line": 20,
          "line_range": [
            20,
            21
          ],
          "target_line_range": [
            22,
            26
          ]
        },
        {
          "code": "class Response(BaseModel):\n    response: str\n    good_response: bool\n\n\n",
          "display_code": "class Response(BaseModel):\n    response: str\n    good_response: bool\n\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 22,
          "line_range": [
            22,
            26
          ]
        },
        {
          "code": "# We'll use the same prompt as before, but this time we'll specify that the\n# response should be a JSON object that matches the Response model.\n",
          "display_code": "",
          "annotation": "We'll use the same prompt as before, but this time we'll specify that the\nresponse should be a JSON object that matches the Response model.",
          "is_comment": true,
          "start_line": 27,
          "line_range": [
            27,
            28
          ],
          "target_line_range": [
            29,
            37
          ]
        },
        {
          "code": "response = completion(\n    model=\"gemini/gemini-2.0-flash-lite\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello what is your name?\"}],\n    response_format={\n        \"type\": \"json_object\",\n        \"response_schema\": Response.model_json_schema(),\n    },\n)\n\n",
          "display_code": "response = completion(\n    model=\"gemini/gemini-2.0-flash-lite\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello what is your name?\"}],\n    response_format={\n        \"type\": \"json_object\",\n        \"response_schema\": Response.model_json_schema(),\n    },\n)\n\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 29,
          "line_range": [
            29,
            37
          ]
        },
        {
          "code": "# The response is a JSON object that matches the Response model.\n",
          "display_code": "",
          "annotation": "The response is a JSON object that matches the Response model.",
          "is_comment": true,
          "start_line": 38,
          "line_range": [
            38,
            38
          ],
          "target_line_range": [
            39,
            39
          ]
        },
        {
          "code": "print(json.loads(response.choices[0].message.content))\n",
          "display_code": "print(json.loads(response.choices[0].message.content))\n",
          "annotation": "",
          "is_comment": false,
          "start_line": 39,
          "line_range": [
            39,
            39
          ]
        }
      ],
      "shell_segments": [
        {
          "explanation": "First, install the LiteLLM and Pydantic libraries",
          "command": "pip install litellm pydantic",
          "output": ""
        },
        {
          "explanation": "Then run the program with Python",
          "command": "python litellm.py",
          "output": "I am a large language model, trained by Google. I don't have a name in the traditional sense. You can just call me by what I am!\n{'good_response': False, 'response': \"I am a large language model, I don't have a name.\"}"
        }
      ],
      "image_data": [],
      "documentation_links": [
        "https://litellm.vercel.app/docs/providers/gemini"
      ],
      "section_id": "008-misc",
      "section_title": "Miscellaneous"
    }
  ],
  "sections": [
    {
      "id": "001-basic-text",
      "title": "Text",
      "description": "",
      "order": 1,
      "examples": [
        "001-basic-generation",
        "002-streaming-text",
        "003-system-prompt",
        "019-reasoning-models",
        "020-structured-output"
      ]
    },
    {
      "id": "002-basic-images",
      "title": "Images",
      "description": "",
      "order": 2,
      "examples": [
        "004-image-q-a",
        "005-image-generation",
        "006-editing-images",
        "007-bounding-boxes",
        "008-image-segmentation"
      ]
    },
    {
      "id": "003-basic-audio",
      "title": "Audio",
      "description": "",
      "order": 3,
      "examples": [
        "009-audio-q-a",
        "010-audio-transcription",
        "011-audio-summarization"
      ]
    },
    {
      "id": "004-basic-video",
      "title": "Video",
      "description": "",
      "order": 4,
      "examples": [
        "012-video-q-a",
        "013-video-summarization",
        "014-video-transcription",
        "015-youtube-video-summarization"
      ]
    },
    {
      "id": "005-other-data-types",
      "title": "PDFs and other data types",
      "description": "",
      "order": 5,
      "examples": [
        "016-pdf-csv-analysis",
        "017-content-translation",
        "018-structured-data-extraction"
      ]
    },
    {
      "id": "006-agentic",
      "title": "Agentic behaviour",
      "description": "",
      "order": 6,
      "examples": [
        "021-tool-use-function-calling",
        "022-code-execution",
        "023-mcp-model-context-protocol",
        "024-grounded-responses"
      ]
    },
    {
      "id": "007-tokens-context-windows",
      "title": "Token counting & context windows",
      "description": "",
      "order": 7,
      "examples": [
        "025-model-context-windows",
        "026-token-counting",
        "027-calculate-input-tokens",
        "028-context-caching"
      ]
    },
    {
      "id": "008-misc",
      "title": "Miscellaneous",
      "description": "",
      "order": 8,
      "examples": [
        "029-rate-limits-retries",
        "030-async-requests",
        "031-embeddings",
        "032-safety-filters",
        "033-litellm"
      ]
    }
  ]
}